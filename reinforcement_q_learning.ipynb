{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Reinforcement Learning (DQN) Tutorial\n",
        "**Author**: [Adam Paszke](https://github.com/apaszke)\n",
        "            [Mark Towers](https://github.com/pseudo-rnd-thoughts)\n",
        "\n",
        "\n",
        "This tutorial shows how to use PyTorch to train a Deep Q Learning (DQN) agent\n",
        "on the CartPole-v1 task from [Gymnasium](https://www.gymnasium.farama.org)_.\n",
        "\n",
        "**Task**\n",
        "\n",
        "The agent has to decide between two actions - moving the cart left or\n",
        "right - so that the pole attached to it stays upright. You can find more\n",
        "information about the environment and other more challenging environments at\n",
        "[Gymnasium's website](https://gymnasium.farama.org/environments/classic_control/cart_pole/)_.\n",
        "\n",
        ".. figure:: /_static/img/cartpole.gif\n",
        "   :alt: CartPole\n",
        "\n",
        "   CartPole\n",
        "\n",
        "As the agent observes the current state of the environment and chooses\n",
        "an action, the environment *transitions* to a new state, and also\n",
        "returns a reward that indicates the consequences of the action. In this\n",
        "task, rewards are +1 for every incremental timestep and the environment\n",
        "terminates if the pole falls over too far or the cart moves more than 2.4\n",
        "units away from center. This means better performing scenarios will run\n",
        "for longer duration, accumulating larger return.\n",
        "\n",
        "The CartPole task is designed so that the inputs to the agent are 4 real\n",
        "values representing the environment state (position, velocity, etc.).\n",
        "We take these 4 inputs without any scaling and pass them through a \n",
        "small fully-connected network with 2 outputs, one for each action. \n",
        "The network is trained to predict the expected value for each action, \n",
        "given the input state. The action with the highest expected value is \n",
        "then chosen.\n",
        "\n",
        "\n",
        "**Packages**\n",
        "\n",
        "\n",
        "First, let's import needed packages. Firstly, we need\n",
        "[gymnasium](https://gymnasium.farama.org/)_ for the environment,\n",
        "installed by using `pip`. This is a fork of the original OpenAI\n",
        "Gym project and maintained by the same team since Gym v0.19.\n",
        "If you are running this in Google Colab, run:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gymnasium[classic_control] in /home/chandler/.local/lib/python3.8/site-packages (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0; python_version < \"3.10\" in /home/chandler/.local/lib/python3.8/site-packages (from gymnasium[classic_control]) (6.3.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /home/chandler/.local/lib/python3.8/site-packages (from gymnasium[classic_control]) (1.24.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /home/chandler/.local/lib/python3.8/site-packages (from gymnasium[classic_control]) (0.0.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /home/chandler/.local/lib/python3.8/site-packages (from gymnasium[classic_control]) (2.2.1)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /home/chandler/.local/lib/python3.8/site-packages (from gymnasium[classic_control]) (1.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /home/chandler/.local/lib/python3.8/site-packages (from gymnasium[classic_control]) (4.5.0)\n",
            "Requirement already satisfied: pygame==2.1.3; extra == \"classic_control\" in /home/chandler/.local/lib/python3.8/site-packages (from gymnasium[classic_control]) (2.1.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/chandler/.local/lib/python3.8/site-packages (from importlib-metadata>=4.8.0; python_version < \"3.10\"->gymnasium[classic_control]) (3.15.0)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip3 install gymnasium[classic_control]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll also use the following from PyTorch:\n",
        "\n",
        "-  neural networks (``torch.nn``)\n",
        "-  optimization (``torch.optim``)\n",
        "-  automatic differentiation (``torch.autograd``)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import math\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "# if GPU is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replay Memory\n",
        "\n",
        "We'll be using experience replay memory for training our DQN. It stores\n",
        "the transitions that the agent observes, allowing us to reuse this data\n",
        "later. By sampling from it randomly, the transitions that build up a\n",
        "batch are decorrelated. It has been shown that this greatly stabilizes\n",
        "and improves the DQN training procedure.\n",
        "\n",
        "For this, we're going to need two classes:\n",
        "\n",
        "-  ``Transition`` - a named tuple representing a single transition in\n",
        "   our environment. It essentially maps (state, action) pairs\n",
        "   to their (next_state, reward) result, with the state being the\n",
        "   screen difference image as described later on.\n",
        "-  ``ReplayMemory`` - a cyclic buffer of bounded size that holds the\n",
        "   transitions observed recently. It also implements a ``.sample()``\n",
        "   method for selecting a random batch of transitions for training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's define our model. But first, let's quickly recap what a DQN is.\n",
        "\n",
        "## DQN algorithm\n",
        "\n",
        "Our environment is deterministic, so all equations presented here are\n",
        "also formulated deterministically for the sake of simplicity. In the\n",
        "reinforcement learning literature, they would also contain expectations\n",
        "over stochastic transitions in the environment.\n",
        "\n",
        "Our aim will be to train a policy that tries to maximize the discounted,\n",
        "cumulative reward\n",
        "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, where\n",
        "$R_{t_0}$ is also known as the *return*. The discount,\n",
        "$\\gamma$, should be a constant between $0$ and $1$\n",
        "that ensures the sum converges. A lower $\\gamma$ makes \n",
        "rewards from the uncertain far future less important for our agent \n",
        "than the ones in the near future that it can be fairly confident \n",
        "about. It also encourages agents to collect reward closer in time \n",
        "than equivalent rewards that are temporally far away in the future.\n",
        "\n",
        "The main idea behind Q-learning is that if we had a function\n",
        "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, that could tell\n",
        "us what our return would be, if we were to take an action in a given\n",
        "state, then we could easily construct a policy that maximizes our\n",
        "rewards:\n",
        "\n",
        "\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}\n",
        "\n",
        "However, we don't know everything about the world, so we don't have\n",
        "access to $Q^*$. But, since neural networks are universal function\n",
        "approximators, we can simply create one and train it to resemble\n",
        "$Q^*$.\n",
        "\n",
        "For our training update rule, we'll use a fact that every $Q$\n",
        "function for some policy obeys the Bellman equation:\n",
        "\n",
        "\\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align}\n",
        "\n",
        "The difference between the two sides of the equality is known as the\n",
        "temporal difference error, $\\delta$:\n",
        "\n",
        "\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a' Q(s', a))\\end{align}\n",
        "\n",
        "To minimize this error, we will use the [Huber\n",
        "loss](https://en.wikipedia.org/wiki/Huber_loss)_. The Huber loss acts\n",
        "like the mean squared error when the error is small, but like the mean\n",
        "absolute error when the error is large - this makes it more robust to\n",
        "outliers when the estimates of $Q$ are very noisy. We calculate\n",
        "this over a batch of transitions, $B$, sampled from the replay\n",
        "memory:\n",
        "\n",
        "\\begin{align}\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\\end{align}\n",
        "\n",
        "\\begin{align}\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n",
        "     \\frac{1}{2}{\\delta^2}  & \\text{for } |\\delta| \\le 1, \\\\\n",
        "     |\\delta| - \\frac{1}{2} & \\text{otherwise.}\n",
        "   \\end{cases}\\end{align}\n",
        "\n",
        "### Q-network\n",
        "\n",
        "Our model will be a feed forward  neural network that takes in the\n",
        "difference between the current and previous screen patches. It has two\n",
        "outputs, representing $Q(s, \\mathrm{left})$ and\n",
        "$Q(s, \\mathrm{right})$ (where $s$ is the input to the\n",
        "network). In effect, the network is trying to predict the *expected return* of\n",
        "taking each action given the current input.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.layer1 = nn.Linear(n_observations, 128)\n",
        "        self.layer2 = nn.Linear(128, 128)\n",
        "        self.layer3 = nn.Linear(128, n_actions)\n",
        "\n",
        "    # Called with either one element to determine next action, or a batch\n",
        "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        return self.layer3(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n",
        "\n",
        "### Hyperparameters and utilities\n",
        "This cell instantiates our model and its optimizer, and defines some\n",
        "utilities:\n",
        "\n",
        "-  ``select_action`` - will select an action accordingly to an epsilon\n",
        "   greedy policy. Simply put, we'll sometimes use our model for choosing\n",
        "   the action, and sometimes we'll just sample one uniformly. The\n",
        "   probability of choosing a random action will start at ``EPS_START``\n",
        "   and will decay exponentially towards ``EPS_END``. ``EPS_DECAY``\n",
        "   controls the rate of the decay.\n",
        "-  ``plot_durations`` - a helper for plotting the duration of episodes,\n",
        "   along with an average over the last 100 episodes (the measure used in\n",
        "   the official evaluations). The plot will be underneath the cell\n",
        "   containing the main training loop, and will update after every\n",
        "   episode.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
        "# GAMMA is the discount factor as mentioned in the previous section\n",
        "# EPS_START is the starting value of epsilon\n",
        "# EPS_END is the final value of epsilon\n",
        "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
        "# TAU is the update rate of the target network\n",
        "# LR is the learning rate of the ``AdamW`` optimizer\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 1000\n",
        "TAU = 0.005\n",
        "LR = 1e-4\n",
        "\n",
        "# Get number of actions from gym action space\n",
        "n_actions = env.action_space.n\n",
        "# Get the number of state observations\n",
        "state, info = env.reset()\n",
        "n_observations = len(state)\n",
        "\n",
        "policy_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            # t.max(1) will return the largest column value of each row.\n",
        "            # second column on max result is index of where max element was\n",
        "            # found, so we pick action with the larger expected reward.\n",
        "            return policy_net(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "episode_durations = []\n",
        "\n",
        "\n",
        "def plot_durations(show_result=False):\n",
        "    plt.figure(1)\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    if show_result:\n",
        "        plt.title('Result')\n",
        "    else:\n",
        "        plt.clf()\n",
        "        plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # Take 100 episode averages and plot them too\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    if is_ipython:\n",
        "        if not show_result:\n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "        else:\n",
        "            display.display(plt.gcf())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training loop\n",
        "\n",
        "Finally, the code for training our model.\n",
        "\n",
        "Here, you can find an ``optimize_model`` function that performs a\n",
        "single step of the optimization. It first samples a batch, concatenates\n",
        "all the tensors into a single one, computes $Q(s_t, a_t)$ and\n",
        "$V(s_{t+1}) = \\max_a Q(s_{t+1}, a)$, and combines them into our\n",
        "loss. By definition we set $V(s) = 0$ if $s$ is a terminal\n",
        "state. We also use a target network to compute $V(s_{t+1})$ for\n",
        "added stability. The target network is updated at every step with a \n",
        "[soft update](https://arxiv.org/pdf/1509.02971.pdf)_ controlled by \n",
        "the hyperparameter ``TAU``, which was previously defined.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    \n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    with torch.no_grad():\n",
        "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "\n",
        "    # Compute Huber loss\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    # In-place gradient clipping\n",
        "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below, you can find the main training loop. At the beginning we reset\n",
        "the environment and obtain the initial ``state`` Tensor. Then, we sample\n",
        "an action, execute it, observe the next state and the reward (always\n",
        "1), and optimize our model once. When the episode ends (our model\n",
        "fails), we restart the loop.\n",
        "\n",
        "Below, `num_episodes` is set to 600 if a GPU is available, otherwise 50 \n",
        "episodes are scheduled so training does not take too long. However, 50 \n",
        "episodes is insufficient for to observe good performance on CartPole.\n",
        "You should see the model constantly achieve 500 steps within 600 training \n",
        "episodes. Training RL agents can be a noisy process, so restarting training\n",
        "can produce better results if convergence is not observed.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Transition(state=tensor([[-0.0044,  0.2352,  0.0520, -0.2604]]), action=tensor([[0]]), next_state=tensor([[0.0003, 0.0394, 0.0468, 0.0483]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0220,  0.4161, -0.0182, -0.5570]]), action=tensor([[0]]), next_state=tensor([[ 0.0304,  0.2212, -0.0293, -0.2701]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0374,  0.6206, -0.0501, -1.0585]]), action=tensor([[1]]), next_state=tensor([[ 0.0499,  0.8164, -0.0713, -1.3665]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0664,  1.1429, -0.0949, -1.7456]]), action=tensor([[0]]), next_state=tensor([[ 0.0893,  0.9490, -0.1298, -1.4838]]), reward=tensor([1.])), Transition(state=tensor([[-0.0070, -0.0291,  0.0144,  0.0447]]), action=tensor([[1]]), next_state=tensor([[-0.0075,  0.1658,  0.0153, -0.2434]]), reward=tensor([1.])), Transition(state=tensor([[0.0003, 0.0394, 0.0468, 0.0483]]), action=tensor([[0]]), next_state=tensor([[ 0.0011, -0.1564,  0.0478,  0.3553]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0348,  0.0265, -0.0347,  0.0132]]), action=tensor([[0]]), next_state=tensor([[ 0.0353, -0.1681, -0.0345,  0.2947]]), reward=tensor([1.])), Transition(state=tensor([[ 0.1120,  1.1490, -0.1938, -1.9633]]), action=tensor([[1]]), next_state=None, reward=tensor([1.])), Transition(state=tensor([[ 0.0376,  0.5968, -0.0475, -0.8430]]), action=tensor([[0]]), next_state=tensor([[ 0.0495,  0.4023, -0.0643, -0.5656]]), reward=tensor([1.])), Transition(state=tensor([[-0.0314,  0.0140,  0.0464, -0.0220]]), action=tensor([[1]]), next_state=tensor([[-0.0311,  0.2084,  0.0460, -0.2997]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0015,  0.4162,  0.0100, -0.5598]]), action=tensor([[1]]), next_state=tensor([[ 0.0098,  0.6112, -0.0012, -0.8493]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0141,  0.3606, -0.0166, -0.5279]]), action=tensor([[1]]), next_state=tensor([[ 0.0213,  0.5559, -0.0271, -0.8258]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0495,  0.4023, -0.0643, -0.5656]]), action=tensor([[0]]), next_state=tensor([[ 0.0575,  0.2082, -0.0756, -0.2939]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0176,  0.7519, -0.0375, -1.1977]]), action=tensor([[1]]), next_state=tensor([[ 0.0326,  0.9475, -0.0615, -1.5019]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0049, -0.3482,  0.0343,  0.5751]]), action=tensor([[1]]), next_state=tensor([[-0.0021, -0.1536,  0.0458,  0.2934]]), reward=tensor([1.])), Transition(state=tensor([[-0.0189,  0.2072,  0.0285, -0.2725]]), action=tensor([[1]]), next_state=tensor([[-0.0147,  0.4019,  0.0230, -0.5561]]), reward=tensor([1.])), Transition(state=tensor([[-0.0091, -0.5480,  0.0681,  0.9721]]), action=tensor([[1]]), next_state=tensor([[-0.0200, -0.3539,  0.0876,  0.7016]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0011, -0.1564,  0.0478,  0.3553]]), action=tensor([[0]]), next_state=tensor([[-0.0020, -0.3522,  0.0549,  0.6627]]), reward=tensor([1.])), Transition(state=tensor([[-0.0020, -0.3522,  0.0549,  0.6627]]), action=tensor([[0]]), next_state=tensor([[-0.0091, -0.5480,  0.0681,  0.9721]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0213,  0.5899, -0.0606, -0.8856]]), action=tensor([[0]]), next_state=tensor([[ 0.0331,  0.3957, -0.0783, -0.6126]]), reward=tensor([1.])), Transition(state=tensor([[-0.0147,  0.4019,  0.0230, -0.5561]]), action=tensor([[0]]), next_state=tensor([[-0.0067,  0.2065,  0.0119, -0.2562]]), reward=tensor([1.])), Transition(state=tensor([[0.0001, 0.0261, 0.0003, 0.0223]]), action=tensor([[1]]), next_state=tensor([[ 0.0007,  0.2212,  0.0008, -0.2702]]), reward=tensor([1.])), Transition(state=tensor([[-0.0052,  0.0408,  0.0517,  0.0156]]), action=tensor([[1]]), next_state=tensor([[-0.0044,  0.2352,  0.0520, -0.2604]]), reward=tensor([1.])), Transition(state=tensor([[-0.0063, -0.0288,  0.0058, -0.0214]]), action=tensor([[1]]), next_state=tensor([[-0.0069,  0.1663,  0.0054, -0.3123]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0444, -0.3581, -0.0477,  0.4766]]), action=tensor([[1]]), next_state=tensor([[ 0.0373, -0.1624, -0.0381,  0.1692]]), reward=tensor([1.])), Transition(state=tensor([[-0.0091, -0.0284,  0.0127, -0.0293]]), action=tensor([[1]]), next_state=tensor([[-0.0096,  0.1665,  0.0122, -0.3179]]), reward=tensor([1.])), Transition(state=tensor([[-0.0043,  0.2213,  0.0058, -0.2722]]), action=tensor([[0]]), next_state=tensor([[0.0001, 0.0261, 0.0003, 0.0223]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0269, -0.1613, -0.0258,  0.1462]]), action=tensor([[1]]), next_state=tensor([[ 0.0237,  0.0341, -0.0228, -0.1545]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0170, -0.3469,  0.0134,  0.5450]]), action=tensor([[1]]), next_state=tensor([[ 0.0101, -0.1519,  0.0243,  0.2565]]), reward=tensor([1.])), Transition(state=tensor([[-0.0171,  0.2219,  0.0229, -0.2848]]), action=tensor([[1]]), next_state=tensor([[-0.0126,  0.4167,  0.0172, -0.5702]]), reward=tensor([1.])), Transition(state=tensor([[-0.0102,  0.1981, -0.0154, -0.2638]]), action=tensor([[1]]), next_state=tensor([[-0.0062,  0.3934, -0.0206, -0.5612]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0499,  0.8164, -0.0713, -1.3665]]), action=tensor([[0]]), next_state=tensor([[ 0.0662,  0.6222, -0.0986, -1.0970]]), reward=tensor([1.])), Transition(state=tensor([[-0.0200, -0.3539,  0.0876,  0.7016]]), action=tensor([[0]]), next_state=tensor([[-0.0271, -0.5501,  0.1016,  1.0205]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0016,  0.5888, -0.0319, -0.8604]]), action=tensor([[0]]), next_state=tensor([[ 0.0134,  0.3942, -0.0491, -0.5779]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0064,  0.5566, -0.0195, -0.8990]]), action=tensor([[1]]), next_state=tensor([[ 0.0176,  0.7519, -0.0375, -1.1977]]), reward=tensor([1.])), Transition(state=tensor([[-1.7182e-04,  1.6629e-01,  4.5475e-03, -2.5395e-01]]), action=tensor([[0]]), next_state=tensor([[ 0.0032, -0.0289, -0.0005,  0.0402]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0353, -0.1681, -0.0345,  0.2947]]), action=tensor([[1]]), next_state=tensor([[ 0.0320,  0.0275, -0.0286, -0.0086]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0244,  0.2296, -0.0259, -0.4543]]), action=tensor([[1]]), next_state=tensor([[ 0.0289,  0.4251, -0.0350, -0.7550]]), reward=tensor([1.])), Transition(state=tensor([[ 0.1311,  0.9524, -0.1957, -1.5749]]), action=tensor([[0]]), next_state=None, reward=tensor([1.])), Transition(state=tensor([[-0.0148,  0.0276,  0.0173, -0.0103]]), action=tensor([[0]]), next_state=tensor([[-0.0142, -0.1678,  0.0171,  0.2878]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0375,  0.2240, -0.0355, -0.3302]]), action=tensor([[0]]), next_state=tensor([[ 0.0420,  0.0294, -0.0421, -0.0490]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0410,  0.5918, -0.0906, -0.9288]]), action=tensor([[1]]), next_state=tensor([[ 0.0528,  0.7880, -0.1092, -1.2486]]), reward=tensor([1.])), Transition(state=tensor([[-0.0067,  0.2065,  0.0119, -0.2562]]), action=tensor([[0]]), next_state=tensor([[-0.0026,  0.0112,  0.0068,  0.0402]]), reward=tensor([1.])), Transition(state=tensor([[-0.0281,  0.0152,  0.0423, -0.0483]]), action=tensor([[0]]), next_state=tensor([[-0.0278, -0.1805,  0.0413,  0.2574]]), reward=tensor([1.])), Transition(state=tensor([[-0.0019, -0.0289,  0.0069,  0.0401]]), action=tensor([[0]]), next_state=tensor([[-0.0025, -0.2241,  0.0077,  0.3349]]), reward=tensor([1.])), Transition(state=tensor([[-0.0530, -0.9428,  0.1489,  1.6716]]), action=tensor([[1]]), next_state=tensor([[-0.0719, -0.7497,  0.1823,  1.4287]]), reward=tensor([1.])), Transition(state=tensor([[ 0.1161,  1.0184, -0.1905, -1.8354]]), action=tensor([[1]]), next_state=None, reward=tensor([1.])), Transition(state=tensor([[ 0.0217,  0.7916, -0.0249, -1.1278]]), action=tensor([[0]]), next_state=tensor([[ 0.0376,  0.5968, -0.0475, -0.8430]]), reward=tensor([1.])), Transition(state=tensor([[-0.0126,  0.4167,  0.0172, -0.5702]]), action=tensor([[0]]), next_state=tensor([[-0.0043,  0.2213,  0.0058, -0.2722]]), reward=tensor([1.])), Transition(state=tensor([[-0.0176,  0.0271,  0.0229,  0.0005]]), action=tensor([[1]]), next_state=tensor([[-0.0171,  0.2219,  0.0229, -0.2848]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0331,  0.3957, -0.0783, -0.6126]]), action=tensor([[1]]), next_state=tensor([[ 0.0410,  0.5918, -0.0906, -0.9288]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0662,  0.6222, -0.0986, -1.0970]]), action=tensor([[0]]), next_state=tensor([[ 0.0786,  0.4285, -0.1206, -0.8368]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0477, -0.1638, -0.0517,  0.2006]]), action=tensor([[0]]), next_state=tensor([[ 0.0444, -0.3581, -0.0477,  0.4766]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0617,  0.0142, -0.0815, -0.0260]]), action=tensor([[1]]), next_state=tensor([[ 0.0620,  0.2104, -0.0820, -0.3432]]), reward=tensor([1.])), Transition(state=tensor([[-0.0026,  0.0112,  0.0068,  0.0402]]), action=tensor([[1]]), next_state=tensor([[-0.0024,  0.2062,  0.0076, -0.2503]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0326,  0.9475, -0.0615, -1.5019]]), action=tensor([[0]]), next_state=tensor([[ 0.0516,  0.7532, -0.0915, -1.2290]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0516,  0.7532, -0.0915, -1.2290]]), action=tensor([[0]]), next_state=tensor([[ 0.0666,  0.5594, -0.1161, -0.9664]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0325,  0.2231, -0.0288, -0.3102]]), action=tensor([[0]]), next_state=tensor([[ 0.0370,  0.0283, -0.0350, -0.0267]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0426,  0.2251, -0.0431, -0.3546]]), action=tensor([[0]]), next_state=tensor([[ 0.0471,  0.0306, -0.0502, -0.0758]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0471,  0.0306, -0.0502, -0.0758]]), action=tensor([[0]]), next_state=tensor([[ 0.0477, -0.1638, -0.0517,  0.2006]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0056,  0.2213, -0.0042, -0.2714]]), action=tensor([[0]]), next_state=tensor([[ 0.0100,  0.0262, -0.0096,  0.0199]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0201, -0.1516,  0.0084,  0.2496]]), action=tensor([[0]]), next_state=tensor([[ 0.0170, -0.3469,  0.0134,  0.5450]]), reward=tensor([1.])), Transition(state=tensor([[-0.0069,  0.1663,  0.0054, -0.3123]]), action=tensor([[0]]), next_state=tensor([[-0.0035, -0.0289, -0.0009, -0.0179]]), reward=tensor([1.])), Transition(state=tensor([[ 0.1119,  0.9878, -0.2037, -1.6676]]), action=tensor([[0]]), next_state=None, reward=tensor([1.])), Transition(state=tensor([[ 0.0100,  0.0262, -0.0096,  0.0199]]), action=tensor([[0]]), next_state=tensor([[ 0.0106, -0.1687, -0.0092,  0.3096]]), reward=tensor([1.])), Transition(state=tensor([[-0.0041,  0.1662, -0.0012, -0.3109]]), action=tensor([[1]]), next_state=tensor([[-0.0008,  0.3613, -0.0075, -0.6039]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0324,  0.7514, -0.0437, -1.1269]]), action=tensor([[1]]), next_state=tensor([[ 0.0475,  0.9471, -0.0662, -1.4330]]), reward=tensor([1.])), Transition(state=tensor([[-0.0075,  0.1658,  0.0153, -0.2434]]), action=tensor([[1]]), next_state=tensor([[-0.0042,  0.3607,  0.0105, -0.5312]]), reward=tensor([1.])), Transition(state=tensor([[ 2.9838e-03,  5.5568e-01, -1.7258e-04, -8.2058e-01]]), action=tensor([[0]]), next_state=tensor([[ 0.0141,  0.3606, -0.0166, -0.5279]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0893,  0.9490, -0.1298, -1.4838]]), action=tensor([[1]]), next_state=tensor([[ 0.1082,  1.1455, -0.1594, -1.8141]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0213,  0.5559, -0.0271, -0.8258]]), action=tensor([[1]]), next_state=tensor([[ 0.0324,  0.7514, -0.0437, -1.1269]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0289,  0.4251, -0.0350, -0.7550]]), action=tensor([[1]]), next_state=tensor([[ 0.0374,  0.6206, -0.0501, -1.0585]]), reward=tensor([1.])), Transition(state=tensor([[-0.0096,  0.1665,  0.0122, -0.3179]]), action=tensor([[0]]), next_state=tensor([[-0.0063, -0.0288,  0.0058, -0.0214]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0340, -0.3569, -0.0348,  0.4496]]), action=tensor([[1]]), next_state=tensor([[ 0.0269, -0.1613, -0.0258,  0.1462]]), reward=tensor([1.])), Transition(state=tensor([[-0.0008,  0.3613, -0.0075, -0.6039]]), action=tensor([[1]]), next_state=tensor([[ 0.0064,  0.5566, -0.0195, -0.8990]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0213,  0.0438,  0.0054, -0.0491]]), action=tensor([[0]]), next_state=tensor([[ 0.0222, -0.1514,  0.0044,  0.2453]]), reward=tensor([1.])), Transition(state=tensor([[-0.0035,  0.0264,  0.0151,  0.0159]]), action=tensor([[1]]), next_state=tensor([[-0.0029,  0.2213,  0.0154, -0.2720]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0007,  0.2212,  0.0008, -0.2702]]), action=tensor([[0]]), next_state=tensor([[ 0.0051,  0.0261, -0.0046,  0.0227]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0778,  0.7558, -0.1354, -1.2932]]), action=tensor([[1]]), next_state=tensor([[ 0.0929,  0.9524, -0.1613, -1.6250]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0575,  0.2082, -0.0756, -0.2939]]), action=tensor([[0]]), next_state=tensor([[ 0.0617,  0.0142, -0.0815, -0.0260]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0528,  0.7880, -0.1092, -1.2486]]), action=tensor([[1]]), next_state=tensor([[ 0.0686,  0.9844, -0.1341, -1.5733]]), reward=tensor([1.])), Transition(state=tensor([[-0.0021, -0.1536,  0.0458,  0.2934]]), action=tensor([[1]]), next_state=tensor([[-0.0052,  0.0408,  0.0517,  0.0156]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0070,  0.0428,  0.0294, -0.0284]]), action=tensor([[0]]), next_state=tensor([[ 0.0079, -0.1527,  0.0289,  0.2735]]), reward=tensor([1.])), Transition(state=tensor([[-0.0029,  0.2213,  0.0154, -0.2720]]), action=tensor([[1]]), next_state=tensor([[ 0.0015,  0.4162,  0.0100, -0.5598]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0079, -0.1527,  0.0289,  0.2735]]), action=tensor([[0]]), next_state=tensor([[ 0.0049, -0.3482,  0.0343,  0.5751]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0373, -0.1624, -0.0381,  0.1692]]), action=tensor([[0]]), next_state=tensor([[ 0.0340, -0.3569, -0.0348,  0.4496]]), reward=tensor([1.])), Transition(state=tensor([[-0.0271, -0.5501,  0.1016,  1.0205]]), action=tensor([[0]]), next_state=tensor([[-0.0381, -0.7464,  0.1220,  1.3433]]), reward=tensor([1.])), Transition(state=tensor([[-0.0269,  0.4029,  0.0400, -0.5775]]), action=tensor([[0]]), next_state=tensor([[-0.0189,  0.2072,  0.0285, -0.2725]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0620,  0.2104, -0.0820, -0.3432]]), action=tensor([[1]]), next_state=tensor([[ 0.0662,  0.4066, -0.0889, -0.6606]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0004, -0.0288,  0.0038,  0.0375]]), action=tensor([[1]]), next_state=tensor([[-1.7182e-04,  1.6629e-01,  4.5475e-03, -2.5395e-01]]), reward=tensor([1.])), Transition(state=tensor([[-0.0381, -0.7464,  0.1220,  1.3433]]), action=tensor([[0]]), next_state=tensor([[-0.0530, -0.9428,  0.1489,  1.6716]]), reward=tensor([1.])), Transition(state=tensor([[-0.0025, -0.2241,  0.0077,  0.3349]]), action=tensor([[1]]), next_state=tensor([[-0.0070, -0.0291,  0.0144,  0.0447]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0304,  0.2212, -0.0293, -0.2701]]), action=tensor([[0]]), next_state=tensor([[ 0.0348,  0.0265, -0.0347,  0.0132]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0686,  0.9844, -0.1341, -1.5733]]), action=tensor([[1]]), next_state=tensor([[ 0.0883,  1.1808, -0.1656, -1.9047]]), reward=tensor([1.])), Transition(state=tensor([[-0.0311,  0.2084,  0.0460, -0.2997]]), action=tensor([[1]]), next_state=tensor([[-0.0269,  0.4029,  0.0400, -0.5775]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0475,  0.9471, -0.0662, -1.4330]]), action=tensor([[1]]), next_state=tensor([[ 0.0664,  1.1429, -0.0949, -1.7456]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0883,  1.1808, -0.1656, -1.9047]]), action=tensor([[0]]), next_state=tensor([[ 0.1119,  0.9878, -0.2037, -1.6676]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0106, -0.1687, -0.0092,  0.3096]]), action=tensor([[0]]), next_state=tensor([[ 0.0072, -0.3637, -0.0030,  0.5993]]), reward=tensor([1.])), Transition(state=tensor([[-0.0719, -0.7497,  0.1823,  1.4287]]), action=tensor([[0]]), next_state=None, reward=tensor([1.])), Transition(state=tensor([[ 0.0134,  0.3942, -0.0491, -0.5779]]), action=tensor([[1]]), next_state=tensor([[ 0.0213,  0.5899, -0.0606, -0.8856]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0098,  0.6112, -0.0012, -0.8493]]), action=tensor([[0]]), next_state=tensor([[ 0.0220,  0.4161, -0.0182, -0.5570]]), reward=tensor([1.])), Transition(state=tensor([[-0.0142, -0.1678,  0.0171,  0.2878]]), action=tensor([[1]]), next_state=tensor([[-0.0176,  0.0271,  0.0229,  0.0005]]), reward=tensor([1.])), Transition(state=tensor([[ 2.5760e-03, -2.2401e-01,  2.7161e-04,  3.3268e-01]]), action=tensor([[1]]), next_state=tensor([[-0.0019, -0.0289,  0.0069,  0.0401]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0101, -0.1519,  0.0243,  0.2565]]), action=tensor([[1]]), next_state=tensor([[ 0.0070,  0.0428,  0.0294, -0.0284]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0032, -0.0289, -0.0005,  0.0402]]), action=tensor([[0]]), next_state=tensor([[ 2.5760e-03, -2.2401e-01,  2.7161e-04,  3.3268e-01]]), reward=tensor([1.])), Transition(state=tensor([[-0.0042,  0.3607,  0.0105, -0.5312]]), action=tensor([[1]]), next_state=tensor([[ 2.9838e-03,  5.5568e-01, -1.7258e-04, -8.2058e-01]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0786,  0.4285, -0.1206, -0.8368]]), action=tensor([[1]]), next_state=tensor([[ 0.0872,  0.6251, -0.1373, -1.1648]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0192,  0.0436,  0.0093, -0.0460]]), action=tensor([[0]]), next_state=tensor([[ 0.0201, -0.1516,  0.0084,  0.2496]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0666,  0.5594, -0.1161, -0.9664]]), action=tensor([[1]]), next_state=tensor([[ 0.0778,  0.7558, -0.1354, -1.2932]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0051,  0.0261, -0.0046,  0.0227]]), action=tensor([[1]]), next_state=tensor([[ 0.0056,  0.2213, -0.0042, -0.2714]]), reward=tensor([1.])), Transition(state=tensor([[ 0.1082,  1.1455, -0.1594, -1.8141]]), action=tensor([[0]]), next_state=tensor([[ 0.1311,  0.9524, -0.1957, -1.5749]]), reward=tensor([1.])), Transition(state=tensor([[-0.0035, -0.0289, -0.0009, -0.0179]]), action=tensor([[1]]), next_state=tensor([[-0.0041,  0.1662, -0.0012, -0.3109]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0420,  0.0294, -0.0421, -0.0490]]), action=tensor([[1]]), next_state=tensor([[ 0.0426,  0.2251, -0.0431, -0.3546]]), reward=tensor([1.])), Transition(state=tensor([[-0.0278, -0.1805,  0.0413,  0.2574]]), action=tensor([[1]]), next_state=tensor([[-0.0314,  0.0140,  0.0464, -0.0220]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0018,  0.4012,  0.0026, -0.5406]]), action=tensor([[1]]), next_state=tensor([[ 0.0098,  0.5963, -0.0082, -0.8325]]), reward=tensor([1.])), Transition(state=tensor([[-0.0102,  0.0027, -0.0160,  0.0339]]), action=tensor([[1]]), next_state=tensor([[-0.0102,  0.1981, -0.0154, -0.2638]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0872,  0.6251, -0.1373, -1.1648]]), action=tensor([[1]]), next_state=tensor([[ 0.0997,  0.8217, -0.1606, -1.4972]]), reward=tensor([1.])), Transition(state=tensor([[-8.2103e-05, -1.6857e-01,  8.9900e-03,  3.0572e-01]]), action=tensor([[1]]), next_state=tensor([[-0.0035,  0.0264,  0.0151,  0.0159]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0320,  0.0275, -0.0286, -0.0086]]), action=tensor([[1]]), next_state=tensor([[ 0.0325,  0.2231, -0.0288, -0.3102]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0072, -0.3637, -0.0030,  0.5993]]), action=tensor([[1]]), next_state=tensor([[-8.2103e-05, -1.6857e-01,  8.9900e-03,  3.0572e-01]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0222, -0.1514,  0.0044,  0.2453]]), action=tensor([[1]]), next_state=tensor([[ 0.0192,  0.0436,  0.0093, -0.0460]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0929,  0.9524, -0.1613, -1.6250]]), action=tensor([[1]]), next_state=tensor([[ 0.1120,  1.1490, -0.1938, -1.9633]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0370,  0.0283, -0.0350, -0.0267]]), action=tensor([[1]]), next_state=tensor([[ 0.0375,  0.2240, -0.0355, -0.3302]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0237,  0.0341, -0.0228, -0.1545]]), action=tensor([[1]]), next_state=tensor([[ 0.0244,  0.2296, -0.0259, -0.4543]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0098,  0.5963, -0.0082, -0.8325]]), action=tensor([[1]]), next_state=tensor([[ 0.0217,  0.7916, -0.0249, -1.1278]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0997,  0.8217, -0.1606, -1.4972]]), action=tensor([[1]]), next_state=tensor([[ 0.1161,  1.0184, -0.1905, -1.8354]]), reward=tensor([1.])), Transition(state=tensor([[-0.0024,  0.2062,  0.0076, -0.2503]]), action=tensor([[1]]), next_state=tensor([[ 0.0018,  0.4012,  0.0026, -0.5406]]), reward=tensor([1.])), Transition(state=tensor([[-0.0062,  0.3934, -0.0206, -0.5612]]), action=tensor([[1]]), next_state=tensor([[ 0.0016,  0.5888, -0.0319, -0.8604]]), reward=tensor([1.]))]\n",
            "Transition(state=(tensor([[-0.0044,  0.2352,  0.0520, -0.2604]]), tensor([[ 0.0220,  0.4161, -0.0182, -0.5570]]), tensor([[ 0.0374,  0.6206, -0.0501, -1.0585]]), tensor([[ 0.0664,  1.1429, -0.0949, -1.7456]]), tensor([[-0.0070, -0.0291,  0.0144,  0.0447]]), tensor([[0.0003, 0.0394, 0.0468, 0.0483]]), tensor([[ 0.0348,  0.0265, -0.0347,  0.0132]]), tensor([[ 0.1120,  1.1490, -0.1938, -1.9633]]), tensor([[ 0.0376,  0.5968, -0.0475, -0.8430]]), tensor([[-0.0314,  0.0140,  0.0464, -0.0220]]), tensor([[ 0.0015,  0.4162,  0.0100, -0.5598]]), tensor([[ 0.0141,  0.3606, -0.0166, -0.5279]]), tensor([[ 0.0495,  0.4023, -0.0643, -0.5656]]), tensor([[ 0.0176,  0.7519, -0.0375, -1.1977]]), tensor([[ 0.0049, -0.3482,  0.0343,  0.5751]]), tensor([[-0.0189,  0.2072,  0.0285, -0.2725]]), tensor([[-0.0091, -0.5480,  0.0681,  0.9721]]), tensor([[ 0.0011, -0.1564,  0.0478,  0.3553]]), tensor([[-0.0020, -0.3522,  0.0549,  0.6627]]), tensor([[ 0.0213,  0.5899, -0.0606, -0.8856]]), tensor([[-0.0147,  0.4019,  0.0230, -0.5561]]), tensor([[0.0001, 0.0261, 0.0003, 0.0223]]), tensor([[-0.0052,  0.0408,  0.0517,  0.0156]]), tensor([[-0.0063, -0.0288,  0.0058, -0.0214]]), tensor([[ 0.0444, -0.3581, -0.0477,  0.4766]]), tensor([[-0.0091, -0.0284,  0.0127, -0.0293]]), tensor([[-0.0043,  0.2213,  0.0058, -0.2722]]), tensor([[ 0.0269, -0.1613, -0.0258,  0.1462]]), tensor([[ 0.0170, -0.3469,  0.0134,  0.5450]]), tensor([[-0.0171,  0.2219,  0.0229, -0.2848]]), tensor([[-0.0102,  0.1981, -0.0154, -0.2638]]), tensor([[ 0.0499,  0.8164, -0.0713, -1.3665]]), tensor([[-0.0200, -0.3539,  0.0876,  0.7016]]), tensor([[ 0.0016,  0.5888, -0.0319, -0.8604]]), tensor([[ 0.0064,  0.5566, -0.0195, -0.8990]]), tensor([[-1.7182e-04,  1.6629e-01,  4.5475e-03, -2.5395e-01]]), tensor([[ 0.0353, -0.1681, -0.0345,  0.2947]]), tensor([[ 0.0244,  0.2296, -0.0259, -0.4543]]), tensor([[ 0.1311,  0.9524, -0.1957, -1.5749]]), tensor([[-0.0148,  0.0276,  0.0173, -0.0103]]), tensor([[ 0.0375,  0.2240, -0.0355, -0.3302]]), tensor([[ 0.0410,  0.5918, -0.0906, -0.9288]]), tensor([[-0.0067,  0.2065,  0.0119, -0.2562]]), tensor([[-0.0281,  0.0152,  0.0423, -0.0483]]), tensor([[-0.0019, -0.0289,  0.0069,  0.0401]]), tensor([[-0.0530, -0.9428,  0.1489,  1.6716]]), tensor([[ 0.1161,  1.0184, -0.1905, -1.8354]]), tensor([[ 0.0217,  0.7916, -0.0249, -1.1278]]), tensor([[-0.0126,  0.4167,  0.0172, -0.5702]]), tensor([[-0.0176,  0.0271,  0.0229,  0.0005]]), tensor([[ 0.0331,  0.3957, -0.0783, -0.6126]]), tensor([[ 0.0662,  0.6222, -0.0986, -1.0970]]), tensor([[ 0.0477, -0.1638, -0.0517,  0.2006]]), tensor([[ 0.0617,  0.0142, -0.0815, -0.0260]]), tensor([[-0.0026,  0.0112,  0.0068,  0.0402]]), tensor([[ 0.0326,  0.9475, -0.0615, -1.5019]]), tensor([[ 0.0516,  0.7532, -0.0915, -1.2290]]), tensor([[ 0.0325,  0.2231, -0.0288, -0.3102]]), tensor([[ 0.0426,  0.2251, -0.0431, -0.3546]]), tensor([[ 0.0471,  0.0306, -0.0502, -0.0758]]), tensor([[ 0.0056,  0.2213, -0.0042, -0.2714]]), tensor([[ 0.0201, -0.1516,  0.0084,  0.2496]]), tensor([[-0.0069,  0.1663,  0.0054, -0.3123]]), tensor([[ 0.1119,  0.9878, -0.2037, -1.6676]]), tensor([[ 0.0100,  0.0262, -0.0096,  0.0199]]), tensor([[-0.0041,  0.1662, -0.0012, -0.3109]]), tensor([[ 0.0324,  0.7514, -0.0437, -1.1269]]), tensor([[-0.0075,  0.1658,  0.0153, -0.2434]]), tensor([[ 2.9838e-03,  5.5568e-01, -1.7258e-04, -8.2058e-01]]), tensor([[ 0.0893,  0.9490, -0.1298, -1.4838]]), tensor([[ 0.0213,  0.5559, -0.0271, -0.8258]]), tensor([[ 0.0289,  0.4251, -0.0350, -0.7550]]), tensor([[-0.0096,  0.1665,  0.0122, -0.3179]]), tensor([[ 0.0340, -0.3569, -0.0348,  0.4496]]), tensor([[-0.0008,  0.3613, -0.0075, -0.6039]]), tensor([[ 0.0213,  0.0438,  0.0054, -0.0491]]), tensor([[-0.0035,  0.0264,  0.0151,  0.0159]]), tensor([[ 0.0007,  0.2212,  0.0008, -0.2702]]), tensor([[ 0.0778,  0.7558, -0.1354, -1.2932]]), tensor([[ 0.0575,  0.2082, -0.0756, -0.2939]]), tensor([[ 0.0528,  0.7880, -0.1092, -1.2486]]), tensor([[-0.0021, -0.1536,  0.0458,  0.2934]]), tensor([[ 0.0070,  0.0428,  0.0294, -0.0284]]), tensor([[-0.0029,  0.2213,  0.0154, -0.2720]]), tensor([[ 0.0079, -0.1527,  0.0289,  0.2735]]), tensor([[ 0.0373, -0.1624, -0.0381,  0.1692]]), tensor([[-0.0271, -0.5501,  0.1016,  1.0205]]), tensor([[-0.0269,  0.4029,  0.0400, -0.5775]]), tensor([[ 0.0620,  0.2104, -0.0820, -0.3432]]), tensor([[ 0.0004, -0.0288,  0.0038,  0.0375]]), tensor([[-0.0381, -0.7464,  0.1220,  1.3433]]), tensor([[-0.0025, -0.2241,  0.0077,  0.3349]]), tensor([[ 0.0304,  0.2212, -0.0293, -0.2701]]), tensor([[ 0.0686,  0.9844, -0.1341, -1.5733]]), tensor([[-0.0311,  0.2084,  0.0460, -0.2997]]), tensor([[ 0.0475,  0.9471, -0.0662, -1.4330]]), tensor([[ 0.0883,  1.1808, -0.1656, -1.9047]]), tensor([[ 0.0106, -0.1687, -0.0092,  0.3096]]), tensor([[-0.0719, -0.7497,  0.1823,  1.4287]]), tensor([[ 0.0134,  0.3942, -0.0491, -0.5779]]), tensor([[ 0.0098,  0.6112, -0.0012, -0.8493]]), tensor([[-0.0142, -0.1678,  0.0171,  0.2878]]), tensor([[ 2.5760e-03, -2.2401e-01,  2.7161e-04,  3.3268e-01]]), tensor([[ 0.0101, -0.1519,  0.0243,  0.2565]]), tensor([[ 0.0032, -0.0289, -0.0005,  0.0402]]), tensor([[-0.0042,  0.3607,  0.0105, -0.5312]]), tensor([[ 0.0786,  0.4285, -0.1206, -0.8368]]), tensor([[ 0.0192,  0.0436,  0.0093, -0.0460]]), tensor([[ 0.0666,  0.5594, -0.1161, -0.9664]]), tensor([[ 0.0051,  0.0261, -0.0046,  0.0227]]), tensor([[ 0.1082,  1.1455, -0.1594, -1.8141]]), tensor([[-0.0035, -0.0289, -0.0009, -0.0179]]), tensor([[ 0.0420,  0.0294, -0.0421, -0.0490]]), tensor([[-0.0278, -0.1805,  0.0413,  0.2574]]), tensor([[ 0.0018,  0.4012,  0.0026, -0.5406]]), tensor([[-0.0102,  0.0027, -0.0160,  0.0339]]), tensor([[ 0.0872,  0.6251, -0.1373, -1.1648]]), tensor([[-8.2103e-05, -1.6857e-01,  8.9900e-03,  3.0572e-01]]), tensor([[ 0.0320,  0.0275, -0.0286, -0.0086]]), tensor([[ 0.0072, -0.3637, -0.0030,  0.5993]]), tensor([[ 0.0222, -0.1514,  0.0044,  0.2453]]), tensor([[ 0.0929,  0.9524, -0.1613, -1.6250]]), tensor([[ 0.0370,  0.0283, -0.0350, -0.0267]]), tensor([[ 0.0237,  0.0341, -0.0228, -0.1545]]), tensor([[ 0.0098,  0.5963, -0.0082, -0.8325]]), tensor([[ 0.0997,  0.8217, -0.1606, -1.4972]]), tensor([[-0.0024,  0.2062,  0.0076, -0.2503]]), tensor([[-0.0062,  0.3934, -0.0206, -0.5612]])), action=(tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]])), next_state=(tensor([[0.0003, 0.0394, 0.0468, 0.0483]]), tensor([[ 0.0304,  0.2212, -0.0293, -0.2701]]), tensor([[ 0.0499,  0.8164, -0.0713, -1.3665]]), tensor([[ 0.0893,  0.9490, -0.1298, -1.4838]]), tensor([[-0.0075,  0.1658,  0.0153, -0.2434]]), tensor([[ 0.0011, -0.1564,  0.0478,  0.3553]]), tensor([[ 0.0353, -0.1681, -0.0345,  0.2947]]), None, tensor([[ 0.0495,  0.4023, -0.0643, -0.5656]]), tensor([[-0.0311,  0.2084,  0.0460, -0.2997]]), tensor([[ 0.0098,  0.6112, -0.0012, -0.8493]]), tensor([[ 0.0213,  0.5559, -0.0271, -0.8258]]), tensor([[ 0.0575,  0.2082, -0.0756, -0.2939]]), tensor([[ 0.0326,  0.9475, -0.0615, -1.5019]]), tensor([[-0.0021, -0.1536,  0.0458,  0.2934]]), tensor([[-0.0147,  0.4019,  0.0230, -0.5561]]), tensor([[-0.0200, -0.3539,  0.0876,  0.7016]]), tensor([[-0.0020, -0.3522,  0.0549,  0.6627]]), tensor([[-0.0091, -0.5480,  0.0681,  0.9721]]), tensor([[ 0.0331,  0.3957, -0.0783, -0.6126]]), tensor([[-0.0067,  0.2065,  0.0119, -0.2562]]), tensor([[ 0.0007,  0.2212,  0.0008, -0.2702]]), tensor([[-0.0044,  0.2352,  0.0520, -0.2604]]), tensor([[-0.0069,  0.1663,  0.0054, -0.3123]]), tensor([[ 0.0373, -0.1624, -0.0381,  0.1692]]), tensor([[-0.0096,  0.1665,  0.0122, -0.3179]]), tensor([[0.0001, 0.0261, 0.0003, 0.0223]]), tensor([[ 0.0237,  0.0341, -0.0228, -0.1545]]), tensor([[ 0.0101, -0.1519,  0.0243,  0.2565]]), tensor([[-0.0126,  0.4167,  0.0172, -0.5702]]), tensor([[-0.0062,  0.3934, -0.0206, -0.5612]]), tensor([[ 0.0662,  0.6222, -0.0986, -1.0970]]), tensor([[-0.0271, -0.5501,  0.1016,  1.0205]]), tensor([[ 0.0134,  0.3942, -0.0491, -0.5779]]), tensor([[ 0.0176,  0.7519, -0.0375, -1.1977]]), tensor([[ 0.0032, -0.0289, -0.0005,  0.0402]]), tensor([[ 0.0320,  0.0275, -0.0286, -0.0086]]), tensor([[ 0.0289,  0.4251, -0.0350, -0.7550]]), None, tensor([[-0.0142, -0.1678,  0.0171,  0.2878]]), tensor([[ 0.0420,  0.0294, -0.0421, -0.0490]]), tensor([[ 0.0528,  0.7880, -0.1092, -1.2486]]), tensor([[-0.0026,  0.0112,  0.0068,  0.0402]]), tensor([[-0.0278, -0.1805,  0.0413,  0.2574]]), tensor([[-0.0025, -0.2241,  0.0077,  0.3349]]), tensor([[-0.0719, -0.7497,  0.1823,  1.4287]]), None, tensor([[ 0.0376,  0.5968, -0.0475, -0.8430]]), tensor([[-0.0043,  0.2213,  0.0058, -0.2722]]), tensor([[-0.0171,  0.2219,  0.0229, -0.2848]]), tensor([[ 0.0410,  0.5918, -0.0906, -0.9288]]), tensor([[ 0.0786,  0.4285, -0.1206, -0.8368]]), tensor([[ 0.0444, -0.3581, -0.0477,  0.4766]]), tensor([[ 0.0620,  0.2104, -0.0820, -0.3432]]), tensor([[-0.0024,  0.2062,  0.0076, -0.2503]]), tensor([[ 0.0516,  0.7532, -0.0915, -1.2290]]), tensor([[ 0.0666,  0.5594, -0.1161, -0.9664]]), tensor([[ 0.0370,  0.0283, -0.0350, -0.0267]]), tensor([[ 0.0471,  0.0306, -0.0502, -0.0758]]), tensor([[ 0.0477, -0.1638, -0.0517,  0.2006]]), tensor([[ 0.0100,  0.0262, -0.0096,  0.0199]]), tensor([[ 0.0170, -0.3469,  0.0134,  0.5450]]), tensor([[-0.0035, -0.0289, -0.0009, -0.0179]]), None, tensor([[ 0.0106, -0.1687, -0.0092,  0.3096]]), tensor([[-0.0008,  0.3613, -0.0075, -0.6039]]), tensor([[ 0.0475,  0.9471, -0.0662, -1.4330]]), tensor([[-0.0042,  0.3607,  0.0105, -0.5312]]), tensor([[ 0.0141,  0.3606, -0.0166, -0.5279]]), tensor([[ 0.1082,  1.1455, -0.1594, -1.8141]]), tensor([[ 0.0324,  0.7514, -0.0437, -1.1269]]), tensor([[ 0.0374,  0.6206, -0.0501, -1.0585]]), tensor([[-0.0063, -0.0288,  0.0058, -0.0214]]), tensor([[ 0.0269, -0.1613, -0.0258,  0.1462]]), tensor([[ 0.0064,  0.5566, -0.0195, -0.8990]]), tensor([[ 0.0222, -0.1514,  0.0044,  0.2453]]), tensor([[-0.0029,  0.2213,  0.0154, -0.2720]]), tensor([[ 0.0051,  0.0261, -0.0046,  0.0227]]), tensor([[ 0.0929,  0.9524, -0.1613, -1.6250]]), tensor([[ 0.0617,  0.0142, -0.0815, -0.0260]]), tensor([[ 0.0686,  0.9844, -0.1341, -1.5733]]), tensor([[-0.0052,  0.0408,  0.0517,  0.0156]]), tensor([[ 0.0079, -0.1527,  0.0289,  0.2735]]), tensor([[ 0.0015,  0.4162,  0.0100, -0.5598]]), tensor([[ 0.0049, -0.3482,  0.0343,  0.5751]]), tensor([[ 0.0340, -0.3569, -0.0348,  0.4496]]), tensor([[-0.0381, -0.7464,  0.1220,  1.3433]]), tensor([[-0.0189,  0.2072,  0.0285, -0.2725]]), tensor([[ 0.0662,  0.4066, -0.0889, -0.6606]]), tensor([[-1.7182e-04,  1.6629e-01,  4.5475e-03, -2.5395e-01]]), tensor([[-0.0530, -0.9428,  0.1489,  1.6716]]), tensor([[-0.0070, -0.0291,  0.0144,  0.0447]]), tensor([[ 0.0348,  0.0265, -0.0347,  0.0132]]), tensor([[ 0.0883,  1.1808, -0.1656, -1.9047]]), tensor([[-0.0269,  0.4029,  0.0400, -0.5775]]), tensor([[ 0.0664,  1.1429, -0.0949, -1.7456]]), tensor([[ 0.1119,  0.9878, -0.2037, -1.6676]]), tensor([[ 0.0072, -0.3637, -0.0030,  0.5993]]), None, tensor([[ 0.0213,  0.5899, -0.0606, -0.8856]]), tensor([[ 0.0220,  0.4161, -0.0182, -0.5570]]), tensor([[-0.0176,  0.0271,  0.0229,  0.0005]]), tensor([[-0.0019, -0.0289,  0.0069,  0.0401]]), tensor([[ 0.0070,  0.0428,  0.0294, -0.0284]]), tensor([[ 2.5760e-03, -2.2401e-01,  2.7161e-04,  3.3268e-01]]), tensor([[ 2.9838e-03,  5.5568e-01, -1.7258e-04, -8.2058e-01]]), tensor([[ 0.0872,  0.6251, -0.1373, -1.1648]]), tensor([[ 0.0201, -0.1516,  0.0084,  0.2496]]), tensor([[ 0.0778,  0.7558, -0.1354, -1.2932]]), tensor([[ 0.0056,  0.2213, -0.0042, -0.2714]]), tensor([[ 0.1311,  0.9524, -0.1957, -1.5749]]), tensor([[-0.0041,  0.1662, -0.0012, -0.3109]]), tensor([[ 0.0426,  0.2251, -0.0431, -0.3546]]), tensor([[-0.0314,  0.0140,  0.0464, -0.0220]]), tensor([[ 0.0098,  0.5963, -0.0082, -0.8325]]), tensor([[-0.0102,  0.1981, -0.0154, -0.2638]]), tensor([[ 0.0997,  0.8217, -0.1606, -1.4972]]), tensor([[-0.0035,  0.0264,  0.0151,  0.0159]]), tensor([[ 0.0325,  0.2231, -0.0288, -0.3102]]), tensor([[-8.2103e-05, -1.6857e-01,  8.9900e-03,  3.0572e-01]]), tensor([[ 0.0192,  0.0436,  0.0093, -0.0460]]), tensor([[ 0.1120,  1.1490, -0.1938, -1.9633]]), tensor([[ 0.0375,  0.2240, -0.0355, -0.3302]]), tensor([[ 0.0244,  0.2296, -0.0259, -0.4543]]), tensor([[ 0.0217,  0.7916, -0.0249, -1.1278]]), tensor([[ 0.1161,  1.0184, -0.1905, -1.8354]]), tensor([[ 0.0018,  0.4012,  0.0026, -0.5406]]), tensor([[ 0.0016,  0.5888, -0.0319, -0.8604]])), reward=(tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.])))\n",
            "[Transition(state=tensor([[ 0.0201, -0.1516,  0.0084,  0.2496]]), action=tensor([[0]]), next_state=tensor([[ 0.0170, -0.3469,  0.0134,  0.5450]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0325,  0.2231, -0.0288, -0.3102]]), action=tensor([[0]]), next_state=tensor([[ 0.0370,  0.0283, -0.0350, -0.0267]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0213,  0.5559, -0.0271, -0.8258]]), action=tensor([[1]]), next_state=tensor([[ 0.0324,  0.7514, -0.0437, -1.1269]]), reward=tensor([1.])), Transition(state=tensor([[ 0.1120,  1.1490, -0.1938, -1.9633]]), action=tensor([[1]]), next_state=None, reward=tensor([1.])), Transition(state=tensor([[-0.0719, -0.7497,  0.1823,  1.4287]]), action=tensor([[0]]), next_state=None, reward=tensor([1.])), Transition(state=tensor([[ 0.0997,  0.8217, -0.1606, -1.4972]]), action=tensor([[1]]), next_state=tensor([[ 0.1161,  1.0184, -0.1905, -1.8354]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0786,  0.4285, -0.1206, -0.8368]]), action=tensor([[1]]), next_state=tensor([[ 0.0872,  0.6251, -0.1373, -1.1648]]), reward=tensor([1.])), Transition(state=tensor([[-0.0025, -0.2241,  0.0077,  0.3349]]), action=tensor([[1]]), next_state=tensor([[-0.0070, -0.0291,  0.0144,  0.0447]]), reward=tensor([1.])), Transition(state=tensor([[-0.0126,  0.4167,  0.0172, -0.5702]]), action=tensor([[0]]), next_state=tensor([[-0.0043,  0.2213,  0.0058, -0.2722]]), reward=tensor([1.])), Transition(state=tensor([[-0.0024,  0.2062,  0.0076, -0.2503]]), action=tensor([[1]]), next_state=tensor([[ 0.0018,  0.4012,  0.0026, -0.5406]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0070,  0.0428,  0.0294, -0.0284]]), action=tensor([[0]]), next_state=tensor([[ 0.0079, -0.1527,  0.0289,  0.2735]]), reward=tensor([1.])), Transition(state=tensor([[ 0.1119,  0.9878, -0.2037, -1.6676]]), action=tensor([[0]]), next_state=None, reward=tensor([1.])), Transition(state=tensor([[-1.7182e-04,  1.6629e-01,  4.5475e-03, -2.5395e-01]]), action=tensor([[0]]), next_state=tensor([[ 0.0032, -0.0289, -0.0005,  0.0402]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0134,  0.3942, -0.0491, -0.5779]]), action=tensor([[1]]), next_state=tensor([[ 0.0213,  0.5899, -0.0606, -0.8856]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0098,  0.6112, -0.0012, -0.8493]]), action=tensor([[0]]), next_state=tensor([[ 0.0220,  0.4161, -0.0182, -0.5570]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0100,  0.0262, -0.0096,  0.0199]]), action=tensor([[0]]), next_state=tensor([[ 0.0106, -0.1687, -0.0092,  0.3096]]), reward=tensor([1.])), Transition(state=tensor([[-0.0067,  0.2065,  0.0119, -0.2562]]), action=tensor([[0]]), next_state=tensor([[-0.0026,  0.0112,  0.0068,  0.0402]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0016,  0.5888, -0.0319, -0.8604]]), action=tensor([[0]]), next_state=tensor([[ 0.0134,  0.3942, -0.0491, -0.5779]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0499,  0.8164, -0.0713, -1.3665]]), action=tensor([[0]]), next_state=tensor([[ 0.0662,  0.6222, -0.0986, -1.0970]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0101, -0.1519,  0.0243,  0.2565]]), action=tensor([[1]]), next_state=tensor([[ 0.0070,  0.0428,  0.0294, -0.0284]]), reward=tensor([1.])), Transition(state=tensor([[-0.0035, -0.0289, -0.0009, -0.0179]]), action=tensor([[1]]), next_state=tensor([[-0.0041,  0.1662, -0.0012, -0.3109]]), reward=tensor([1.])), Transition(state=tensor([[-0.0070, -0.0291,  0.0144,  0.0447]]), action=tensor([[1]]), next_state=tensor([[-0.0075,  0.1658,  0.0153, -0.2434]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0176,  0.7519, -0.0375, -1.1977]]), action=tensor([[1]]), next_state=tensor([[ 0.0326,  0.9475, -0.0615, -1.5019]]), reward=tensor([1.])), Transition(state=tensor([[-0.0069,  0.1663,  0.0054, -0.3123]]), action=tensor([[0]]), next_state=tensor([[-0.0035, -0.0289, -0.0009, -0.0179]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0289,  0.4251, -0.0350, -0.7550]]), action=tensor([[1]]), next_state=tensor([[ 0.0374,  0.6206, -0.0501, -1.0585]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0348,  0.0265, -0.0347,  0.0132]]), action=tensor([[0]]), next_state=tensor([[ 0.0353, -0.1681, -0.0345,  0.2947]]), reward=tensor([1.])), Transition(state=tensor([[-0.0102,  0.1981, -0.0154, -0.2638]]), action=tensor([[1]]), next_state=tensor([[-0.0062,  0.3934, -0.0206, -0.5612]]), reward=tensor([1.])), Transition(state=tensor([[-0.0271, -0.5501,  0.1016,  1.0205]]), action=tensor([[0]]), next_state=tensor([[-0.0381, -0.7464,  0.1220,  1.3433]]), reward=tensor([1.])), Transition(state=tensor([[-0.0008,  0.3613, -0.0075, -0.6039]]), action=tensor([[1]]), next_state=tensor([[ 0.0064,  0.5566, -0.0195, -0.8990]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0222, -0.1514,  0.0044,  0.2453]]), action=tensor([[1]]), next_state=tensor([[ 0.0192,  0.0436,  0.0093, -0.0460]]), reward=tensor([1.])), Transition(state=tensor([[-0.0278, -0.1805,  0.0413,  0.2574]]), action=tensor([[1]]), next_state=tensor([[-0.0314,  0.0140,  0.0464, -0.0220]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0340, -0.3569, -0.0348,  0.4496]]), action=tensor([[1]]), next_state=tensor([[ 0.0269, -0.1613, -0.0258,  0.1462]]), reward=tensor([1.])), Transition(state=tensor([[-0.0142, -0.1678,  0.0171,  0.2878]]), action=tensor([[1]]), next_state=tensor([[-0.0176,  0.0271,  0.0229,  0.0005]]), reward=tensor([1.])), Transition(state=tensor([[-0.0020, -0.3522,  0.0549,  0.6627]]), action=tensor([[0]]), next_state=tensor([[-0.0091, -0.5480,  0.0681,  0.9721]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0620,  0.2104, -0.0820, -0.3432]]), action=tensor([[1]]), next_state=tensor([[ 0.0662,  0.4066, -0.0889, -0.6606]]), reward=tensor([1.])), Transition(state=tensor([[-0.0281,  0.0152,  0.0423, -0.0483]]), action=tensor([[0]]), next_state=tensor([[-0.0278, -0.1805,  0.0413,  0.2574]]), reward=tensor([1.])), Transition(state=tensor([[-0.0381, -0.7464,  0.1220,  1.3433]]), action=tensor([[0]]), next_state=tensor([[-0.0530, -0.9428,  0.1489,  1.6716]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0106, -0.1687, -0.0092,  0.3096]]), action=tensor([[0]]), next_state=tensor([[ 0.0072, -0.3637, -0.0030,  0.5993]]), reward=tensor([1.])), Transition(state=tensor([[-0.0530, -0.9428,  0.1489,  1.6716]]), action=tensor([[1]]), next_state=tensor([[-0.0719, -0.7497,  0.1823,  1.4287]]), reward=tensor([1.])), Transition(state=tensor([[-0.0176,  0.0271,  0.0229,  0.0005]]), action=tensor([[1]]), next_state=tensor([[-0.0171,  0.2219,  0.0229, -0.2848]]), reward=tensor([1.])), Transition(state=tensor([[ 0.1311,  0.9524, -0.1957, -1.5749]]), action=tensor([[0]]), next_state=None, reward=tensor([1.])), Transition(state=tensor([[ 0.0324,  0.7514, -0.0437, -1.1269]]), action=tensor([[1]]), next_state=tensor([[ 0.0475,  0.9471, -0.0662, -1.4330]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0192,  0.0436,  0.0093, -0.0460]]), action=tensor([[0]]), next_state=tensor([[ 0.0201, -0.1516,  0.0084,  0.2496]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0141,  0.3606, -0.0166, -0.5279]]), action=tensor([[1]]), next_state=tensor([[ 0.0213,  0.5559, -0.0271, -0.8258]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0471,  0.0306, -0.0502, -0.0758]]), action=tensor([[0]]), next_state=tensor([[ 0.0477, -0.1638, -0.0517,  0.2006]]), reward=tensor([1.])), Transition(state=tensor([[-0.0019, -0.0289,  0.0069,  0.0401]]), action=tensor([[0]]), next_state=tensor([[-0.0025, -0.2241,  0.0077,  0.3349]]), reward=tensor([1.])), Transition(state=tensor([[-0.0075,  0.1658,  0.0153, -0.2434]]), action=tensor([[1]]), next_state=tensor([[-0.0042,  0.3607,  0.0105, -0.5312]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0376,  0.5968, -0.0475, -0.8430]]), action=tensor([[0]]), next_state=tensor([[ 0.0495,  0.4023, -0.0643, -0.5656]]), reward=tensor([1.])), Transition(state=tensor([[-0.0026,  0.0112,  0.0068,  0.0402]]), action=tensor([[1]]), next_state=tensor([[-0.0024,  0.2062,  0.0076, -0.2503]]), reward=tensor([1.])), Transition(state=tensor([[-0.0044,  0.2352,  0.0520, -0.2604]]), action=tensor([[0]]), next_state=tensor([[0.0003, 0.0394, 0.0468, 0.0483]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0072, -0.3637, -0.0030,  0.5993]]), action=tensor([[1]]), next_state=tensor([[-8.2103e-05, -1.6857e-01,  8.9900e-03,  3.0572e-01]]), reward=tensor([1.])), Transition(state=tensor([[-0.0148,  0.0276,  0.0173, -0.0103]]), action=tensor([[0]]), next_state=tensor([[-0.0142, -0.1678,  0.0171,  0.2878]]), reward=tensor([1.])), Transition(state=tensor([[ 2.5760e-03, -2.2401e-01,  2.7161e-04,  3.3268e-01]]), action=tensor([[1]]), next_state=tensor([[-0.0019, -0.0289,  0.0069,  0.0401]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0662,  0.4066, -0.0889, -0.6606]]), action=tensor([[0]]), next_state=tensor([[ 0.0743,  0.2128, -0.1021, -0.3972]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0237,  0.0341, -0.0228, -0.1545]]), action=tensor([[1]]), next_state=tensor([[ 0.0244,  0.2296, -0.0259, -0.4543]]), reward=tensor([1.])), Transition(state=tensor([[-0.0096,  0.1665,  0.0122, -0.3179]]), action=tensor([[0]]), next_state=tensor([[-0.0063, -0.0288,  0.0058, -0.0214]]), reward=tensor([1.])), Transition(state=tensor([[-0.0043,  0.2213,  0.0058, -0.2722]]), action=tensor([[0]]), next_state=tensor([[0.0001, 0.0261, 0.0003, 0.0223]]), reward=tensor([1.])), Transition(state=tensor([[-0.0041,  0.1662, -0.0012, -0.3109]]), action=tensor([[1]]), next_state=tensor([[-0.0008,  0.3613, -0.0075, -0.6039]]), reward=tensor([1.])), Transition(state=tensor([[-0.0147,  0.4019,  0.0230, -0.5561]]), action=tensor([[0]]), next_state=tensor([[-0.0067,  0.2065,  0.0119, -0.2562]]), reward=tensor([1.])), Transition(state=tensor([[ 0.1161,  1.0184, -0.1905, -1.8354]]), action=tensor([[1]]), next_state=None, reward=tensor([1.])), Transition(state=tensor([[ 0.0004, -0.0288,  0.0038,  0.0375]]), action=tensor([[1]]), next_state=tensor([[-1.7182e-04,  1.6629e-01,  4.5475e-03, -2.5395e-01]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0426,  0.2251, -0.0431, -0.3546]]), action=tensor([[0]]), next_state=tensor([[ 0.0471,  0.0306, -0.0502, -0.0758]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0929,  0.9524, -0.1613, -1.6250]]), action=tensor([[1]]), next_state=tensor([[ 0.1120,  1.1490, -0.1938, -1.9633]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0331,  0.3957, -0.0783, -0.6126]]), action=tensor([[1]]), next_state=tensor([[ 0.0410,  0.5918, -0.0906, -0.9288]]), reward=tensor([1.])), Transition(state=tensor([[-0.0052,  0.0408,  0.0517,  0.0156]]), action=tensor([[1]]), next_state=tensor([[-0.0044,  0.2352,  0.0520, -0.2604]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0893,  0.9490, -0.1298, -1.4838]]), action=tensor([[1]]), next_state=tensor([[ 0.1082,  1.1455, -0.1594, -1.8141]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0666,  0.5594, -0.1161, -0.9664]]), action=tensor([[1]]), next_state=tensor([[ 0.0778,  0.7558, -0.1354, -1.2932]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0007,  0.2212,  0.0008, -0.2702]]), action=tensor([[0]]), next_state=tensor([[ 0.0051,  0.0261, -0.0046,  0.0227]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0410,  0.5918, -0.0906, -0.9288]]), action=tensor([[1]]), next_state=tensor([[ 0.0528,  0.7880, -0.1092, -1.2486]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0664,  1.1429, -0.0949, -1.7456]]), action=tensor([[0]]), next_state=tensor([[ 0.0893,  0.9490, -0.1298, -1.4838]]), reward=tensor([1.])), Transition(state=tensor([[-0.0200, -0.3539,  0.0876,  0.7016]]), action=tensor([[0]]), next_state=tensor([[-0.0271, -0.5501,  0.1016,  1.0205]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0015,  0.4162,  0.0100, -0.5598]]), action=tensor([[1]]), next_state=tensor([[ 0.0098,  0.6112, -0.0012, -0.8493]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0662,  0.6222, -0.0986, -1.0970]]), action=tensor([[0]]), next_state=tensor([[ 0.0786,  0.4285, -0.1206, -0.8368]]), reward=tensor([1.])), Transition(state=tensor([[-8.2103e-05, -1.6857e-01,  8.9900e-03,  3.0572e-01]]), action=tensor([[1]]), next_state=tensor([[-0.0035,  0.0264,  0.0151,  0.0159]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0213,  0.5899, -0.0606, -0.8856]]), action=tensor([[0]]), next_state=tensor([[ 0.0331,  0.3957, -0.0783, -0.6126]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0495,  0.4023, -0.0643, -0.5656]]), action=tensor([[0]]), next_state=tensor([[ 0.0575,  0.2082, -0.0756, -0.2939]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0244,  0.2296, -0.0259, -0.4543]]), action=tensor([[1]]), next_state=tensor([[ 0.0289,  0.4251, -0.0350, -0.7550]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0049, -0.3482,  0.0343,  0.5751]]), action=tensor([[1]]), next_state=tensor([[-0.0021, -0.1536,  0.0458,  0.2934]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0373, -0.1624, -0.0381,  0.1692]]), action=tensor([[0]]), next_state=tensor([[ 0.0340, -0.3569, -0.0348,  0.4496]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0064,  0.5566, -0.0195, -0.8990]]), action=tensor([[1]]), next_state=tensor([[ 0.0176,  0.7519, -0.0375, -1.1977]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0516,  0.7532, -0.0915, -1.2290]]), action=tensor([[0]]), next_state=tensor([[ 0.0666,  0.5594, -0.1161, -0.9664]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0098,  0.5963, -0.0082, -0.8325]]), action=tensor([[1]]), next_state=tensor([[ 0.0217,  0.7916, -0.0249, -1.1278]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0326,  0.9475, -0.0615, -1.5019]]), action=tensor([[0]]), next_state=tensor([[ 0.0516,  0.7532, -0.0915, -1.2290]]), reward=tensor([1.])), Transition(state=tensor([[-0.0102,  0.0027, -0.0160,  0.0339]]), action=tensor([[1]]), next_state=tensor([[-0.0102,  0.1981, -0.0154, -0.2638]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0032, -0.0289, -0.0005,  0.0402]]), action=tensor([[0]]), next_state=tensor([[ 2.5760e-03, -2.2401e-01,  2.7161e-04,  3.3268e-01]]), reward=tensor([1.])), Transition(state=tensor([[-0.0171,  0.2219,  0.0229, -0.2848]]), action=tensor([[1]]), next_state=tensor([[-0.0126,  0.4167,  0.0172, -0.5702]]), reward=tensor([1.])), Transition(state=tensor([[-0.0062,  0.3934, -0.0206, -0.5612]]), action=tensor([[1]]), next_state=tensor([[ 0.0016,  0.5888, -0.0319, -0.8604]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0011, -0.1564,  0.0478,  0.3553]]), action=tensor([[0]]), next_state=tensor([[-0.0020, -0.3522,  0.0549,  0.6627]]), reward=tensor([1.])), Transition(state=tensor([[-0.0269,  0.4029,  0.0400, -0.5775]]), action=tensor([[0]]), next_state=tensor([[-0.0189,  0.2072,  0.0285, -0.2725]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0170, -0.3469,  0.0134,  0.5450]]), action=tensor([[1]]), next_state=tensor([[ 0.0101, -0.1519,  0.0243,  0.2565]]), reward=tensor([1.])), Transition(state=tensor([[-0.0021, -0.1536,  0.0458,  0.2934]]), action=tensor([[1]]), next_state=tensor([[-0.0052,  0.0408,  0.0517,  0.0156]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0778,  0.7558, -0.1354, -1.2932]]), action=tensor([[1]]), next_state=tensor([[ 0.0929,  0.9524, -0.1613, -1.6250]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0686,  0.9844, -0.1341, -1.5733]]), action=tensor([[1]]), next_state=tensor([[ 0.0883,  1.1808, -0.1656, -1.9047]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0475,  0.9471, -0.0662, -1.4330]]), action=tensor([[1]]), next_state=tensor([[ 0.0664,  1.1429, -0.0949, -1.7456]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0883,  1.1808, -0.1656, -1.9047]]), action=tensor([[0]]), next_state=tensor([[ 0.1119,  0.9878, -0.2037, -1.6676]]), reward=tensor([1.])), Transition(state=tensor([[-0.0311,  0.2084,  0.0460, -0.2997]]), action=tensor([[1]]), next_state=tensor([[-0.0269,  0.4029,  0.0400, -0.5775]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0375,  0.2240, -0.0355, -0.3302]]), action=tensor([[0]]), next_state=tensor([[ 0.0420,  0.0294, -0.0421, -0.0490]]), reward=tensor([1.])), Transition(state=tensor([[ 2.9838e-03,  5.5568e-01, -1.7258e-04, -8.2058e-01]]), action=tensor([[0]]), next_state=tensor([[ 0.0141,  0.3606, -0.0166, -0.5279]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0420,  0.0294, -0.0421, -0.0490]]), action=tensor([[1]]), next_state=tensor([[ 0.0426,  0.2251, -0.0431, -0.3546]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0477, -0.1638, -0.0517,  0.2006]]), action=tensor([[0]]), next_state=tensor([[ 0.0444, -0.3581, -0.0477,  0.4766]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0528,  0.7880, -0.1092, -1.2486]]), action=tensor([[1]]), next_state=tensor([[ 0.0686,  0.9844, -0.1341, -1.5733]]), reward=tensor([1.])), Transition(state=tensor([[ 0.1082,  1.1455, -0.1594, -1.8141]]), action=tensor([[0]]), next_state=tensor([[ 0.1311,  0.9524, -0.1957, -1.5749]]), reward=tensor([1.])), Transition(state=tensor([[-0.0035,  0.0264,  0.0151,  0.0159]]), action=tensor([[1]]), next_state=tensor([[-0.0029,  0.2213,  0.0154, -0.2720]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0056,  0.2213, -0.0042, -0.2714]]), action=tensor([[0]]), next_state=tensor([[ 0.0100,  0.0262, -0.0096,  0.0199]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0575,  0.2082, -0.0756, -0.2939]]), action=tensor([[0]]), next_state=tensor([[ 0.0617,  0.0142, -0.0815, -0.0260]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0051,  0.0261, -0.0046,  0.0227]]), action=tensor([[1]]), next_state=tensor([[ 0.0056,  0.2213, -0.0042, -0.2714]]), reward=tensor([1.])), Transition(state=tensor([[-0.0091, -0.5480,  0.0681,  0.9721]]), action=tensor([[1]]), next_state=tensor([[-0.0200, -0.3539,  0.0876,  0.7016]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0872,  0.6251, -0.1373, -1.1648]]), action=tensor([[1]]), next_state=tensor([[ 0.0997,  0.8217, -0.1606, -1.4972]]), reward=tensor([1.])), Transition(state=tensor([[0.0003, 0.0394, 0.0468, 0.0483]]), action=tensor([[0]]), next_state=tensor([[ 0.0011, -0.1564,  0.0478,  0.3553]]), reward=tensor([1.])), Transition(state=tensor([[0.0001, 0.0261, 0.0003, 0.0223]]), action=tensor([[1]]), next_state=tensor([[ 0.0007,  0.2212,  0.0008, -0.2702]]), reward=tensor([1.])), Transition(state=tensor([[-0.0091, -0.0284,  0.0127, -0.0293]]), action=tensor([[1]]), next_state=tensor([[-0.0096,  0.1665,  0.0122, -0.3179]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0304,  0.2212, -0.0293, -0.2701]]), action=tensor([[0]]), next_state=tensor([[ 0.0348,  0.0265, -0.0347,  0.0132]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0374,  0.6206, -0.0501, -1.0585]]), action=tensor([[1]]), next_state=tensor([[ 0.0499,  0.8164, -0.0713, -1.3665]]), reward=tensor([1.])), Transition(state=tensor([[-0.0063, -0.0288,  0.0058, -0.0214]]), action=tensor([[1]]), next_state=tensor([[-0.0069,  0.1663,  0.0054, -0.3123]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0353, -0.1681, -0.0345,  0.2947]]), action=tensor([[1]]), next_state=tensor([[ 0.0320,  0.0275, -0.0286, -0.0086]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0320,  0.0275, -0.0286, -0.0086]]), action=tensor([[1]]), next_state=tensor([[ 0.0325,  0.2231, -0.0288, -0.3102]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0617,  0.0142, -0.0815, -0.0260]]), action=tensor([[1]]), next_state=tensor([[ 0.0620,  0.2104, -0.0820, -0.3432]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0213,  0.0438,  0.0054, -0.0491]]), action=tensor([[0]]), next_state=tensor([[ 0.0222, -0.1514,  0.0044,  0.2453]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0370,  0.0283, -0.0350, -0.0267]]), action=tensor([[1]]), next_state=tensor([[ 0.0375,  0.2240, -0.0355, -0.3302]]), reward=tensor([1.])), Transition(state=tensor([[-0.0314,  0.0140,  0.0464, -0.0220]]), action=tensor([[1]]), next_state=tensor([[-0.0311,  0.2084,  0.0460, -0.2997]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0018,  0.4012,  0.0026, -0.5406]]), action=tensor([[1]]), next_state=tensor([[ 0.0098,  0.5963, -0.0082, -0.8325]]), reward=tensor([1.])), Transition(state=tensor([[-0.0189,  0.2072,  0.0285, -0.2725]]), action=tensor([[1]]), next_state=tensor([[-0.0147,  0.4019,  0.0230, -0.5561]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0444, -0.3581, -0.0477,  0.4766]]), action=tensor([[1]]), next_state=tensor([[ 0.0373, -0.1624, -0.0381,  0.1692]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0269, -0.1613, -0.0258,  0.1462]]), action=tensor([[1]]), next_state=tensor([[ 0.0237,  0.0341, -0.0228, -0.1545]]), reward=tensor([1.])), Transition(state=tensor([[-0.0042,  0.3607,  0.0105, -0.5312]]), action=tensor([[1]]), next_state=tensor([[ 2.9838e-03,  5.5568e-01, -1.7258e-04, -8.2058e-01]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0079, -0.1527,  0.0289,  0.2735]]), action=tensor([[0]]), next_state=tensor([[ 0.0049, -0.3482,  0.0343,  0.5751]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0220,  0.4161, -0.0182, -0.5570]]), action=tensor([[0]]), next_state=tensor([[ 0.0304,  0.2212, -0.0293, -0.2701]]), reward=tensor([1.])), Transition(state=tensor([[-0.0029,  0.2213,  0.0154, -0.2720]]), action=tensor([[1]]), next_state=tensor([[ 0.0015,  0.4162,  0.0100, -0.5598]]), reward=tensor([1.]))]\n",
            "Transition(state=(tensor([[ 0.0201, -0.1516,  0.0084,  0.2496]]), tensor([[ 0.0325,  0.2231, -0.0288, -0.3102]]), tensor([[ 0.0213,  0.5559, -0.0271, -0.8258]]), tensor([[ 0.1120,  1.1490, -0.1938, -1.9633]]), tensor([[-0.0719, -0.7497,  0.1823,  1.4287]]), tensor([[ 0.0997,  0.8217, -0.1606, -1.4972]]), tensor([[ 0.0786,  0.4285, -0.1206, -0.8368]]), tensor([[-0.0025, -0.2241,  0.0077,  0.3349]]), tensor([[-0.0126,  0.4167,  0.0172, -0.5702]]), tensor([[-0.0024,  0.2062,  0.0076, -0.2503]]), tensor([[ 0.0070,  0.0428,  0.0294, -0.0284]]), tensor([[ 0.1119,  0.9878, -0.2037, -1.6676]]), tensor([[-1.7182e-04,  1.6629e-01,  4.5475e-03, -2.5395e-01]]), tensor([[ 0.0134,  0.3942, -0.0491, -0.5779]]), tensor([[ 0.0098,  0.6112, -0.0012, -0.8493]]), tensor([[ 0.0100,  0.0262, -0.0096,  0.0199]]), tensor([[-0.0067,  0.2065,  0.0119, -0.2562]]), tensor([[ 0.0016,  0.5888, -0.0319, -0.8604]]), tensor([[ 0.0499,  0.8164, -0.0713, -1.3665]]), tensor([[ 0.0101, -0.1519,  0.0243,  0.2565]]), tensor([[-0.0035, -0.0289, -0.0009, -0.0179]]), tensor([[-0.0070, -0.0291,  0.0144,  0.0447]]), tensor([[ 0.0176,  0.7519, -0.0375, -1.1977]]), tensor([[-0.0069,  0.1663,  0.0054, -0.3123]]), tensor([[ 0.0289,  0.4251, -0.0350, -0.7550]]), tensor([[ 0.0348,  0.0265, -0.0347,  0.0132]]), tensor([[-0.0102,  0.1981, -0.0154, -0.2638]]), tensor([[-0.0271, -0.5501,  0.1016,  1.0205]]), tensor([[-0.0008,  0.3613, -0.0075, -0.6039]]), tensor([[ 0.0222, -0.1514,  0.0044,  0.2453]]), tensor([[-0.0278, -0.1805,  0.0413,  0.2574]]), tensor([[ 0.0340, -0.3569, -0.0348,  0.4496]]), tensor([[-0.0142, -0.1678,  0.0171,  0.2878]]), tensor([[-0.0020, -0.3522,  0.0549,  0.6627]]), tensor([[ 0.0620,  0.2104, -0.0820, -0.3432]]), tensor([[-0.0281,  0.0152,  0.0423, -0.0483]]), tensor([[-0.0381, -0.7464,  0.1220,  1.3433]]), tensor([[ 0.0106, -0.1687, -0.0092,  0.3096]]), tensor([[-0.0530, -0.9428,  0.1489,  1.6716]]), tensor([[-0.0176,  0.0271,  0.0229,  0.0005]]), tensor([[ 0.1311,  0.9524, -0.1957, -1.5749]]), tensor([[ 0.0324,  0.7514, -0.0437, -1.1269]]), tensor([[ 0.0192,  0.0436,  0.0093, -0.0460]]), tensor([[ 0.0141,  0.3606, -0.0166, -0.5279]]), tensor([[ 0.0471,  0.0306, -0.0502, -0.0758]]), tensor([[-0.0019, -0.0289,  0.0069,  0.0401]]), tensor([[-0.0075,  0.1658,  0.0153, -0.2434]]), tensor([[ 0.0376,  0.5968, -0.0475, -0.8430]]), tensor([[-0.0026,  0.0112,  0.0068,  0.0402]]), tensor([[-0.0044,  0.2352,  0.0520, -0.2604]]), tensor([[ 0.0072, -0.3637, -0.0030,  0.5993]]), tensor([[-0.0148,  0.0276,  0.0173, -0.0103]]), tensor([[ 2.5760e-03, -2.2401e-01,  2.7161e-04,  3.3268e-01]]), tensor([[ 0.0662,  0.4066, -0.0889, -0.6606]]), tensor([[ 0.0237,  0.0341, -0.0228, -0.1545]]), tensor([[-0.0096,  0.1665,  0.0122, -0.3179]]), tensor([[-0.0043,  0.2213,  0.0058, -0.2722]]), tensor([[-0.0041,  0.1662, -0.0012, -0.3109]]), tensor([[-0.0147,  0.4019,  0.0230, -0.5561]]), tensor([[ 0.1161,  1.0184, -0.1905, -1.8354]]), tensor([[ 0.0004, -0.0288,  0.0038,  0.0375]]), tensor([[ 0.0426,  0.2251, -0.0431, -0.3546]]), tensor([[ 0.0929,  0.9524, -0.1613, -1.6250]]), tensor([[ 0.0331,  0.3957, -0.0783, -0.6126]]), tensor([[-0.0052,  0.0408,  0.0517,  0.0156]]), tensor([[ 0.0893,  0.9490, -0.1298, -1.4838]]), tensor([[ 0.0666,  0.5594, -0.1161, -0.9664]]), tensor([[ 0.0007,  0.2212,  0.0008, -0.2702]]), tensor([[ 0.0410,  0.5918, -0.0906, -0.9288]]), tensor([[ 0.0664,  1.1429, -0.0949, -1.7456]]), tensor([[-0.0200, -0.3539,  0.0876,  0.7016]]), tensor([[ 0.0015,  0.4162,  0.0100, -0.5598]]), tensor([[ 0.0662,  0.6222, -0.0986, -1.0970]]), tensor([[-8.2103e-05, -1.6857e-01,  8.9900e-03,  3.0572e-01]]), tensor([[ 0.0213,  0.5899, -0.0606, -0.8856]]), tensor([[ 0.0495,  0.4023, -0.0643, -0.5656]]), tensor([[ 0.0244,  0.2296, -0.0259, -0.4543]]), tensor([[ 0.0049, -0.3482,  0.0343,  0.5751]]), tensor([[ 0.0373, -0.1624, -0.0381,  0.1692]]), tensor([[ 0.0064,  0.5566, -0.0195, -0.8990]]), tensor([[ 0.0516,  0.7532, -0.0915, -1.2290]]), tensor([[ 0.0098,  0.5963, -0.0082, -0.8325]]), tensor([[ 0.0326,  0.9475, -0.0615, -1.5019]]), tensor([[-0.0102,  0.0027, -0.0160,  0.0339]]), tensor([[ 0.0032, -0.0289, -0.0005,  0.0402]]), tensor([[-0.0171,  0.2219,  0.0229, -0.2848]]), tensor([[-0.0062,  0.3934, -0.0206, -0.5612]]), tensor([[ 0.0011, -0.1564,  0.0478,  0.3553]]), tensor([[-0.0269,  0.4029,  0.0400, -0.5775]]), tensor([[ 0.0170, -0.3469,  0.0134,  0.5450]]), tensor([[-0.0021, -0.1536,  0.0458,  0.2934]]), tensor([[ 0.0778,  0.7558, -0.1354, -1.2932]]), tensor([[ 0.0686,  0.9844, -0.1341, -1.5733]]), tensor([[ 0.0475,  0.9471, -0.0662, -1.4330]]), tensor([[ 0.0883,  1.1808, -0.1656, -1.9047]]), tensor([[-0.0311,  0.2084,  0.0460, -0.2997]]), tensor([[ 0.0375,  0.2240, -0.0355, -0.3302]]), tensor([[ 2.9838e-03,  5.5568e-01, -1.7258e-04, -8.2058e-01]]), tensor([[ 0.0420,  0.0294, -0.0421, -0.0490]]), tensor([[ 0.0477, -0.1638, -0.0517,  0.2006]]), tensor([[ 0.0528,  0.7880, -0.1092, -1.2486]]), tensor([[ 0.1082,  1.1455, -0.1594, -1.8141]]), tensor([[-0.0035,  0.0264,  0.0151,  0.0159]]), tensor([[ 0.0056,  0.2213, -0.0042, -0.2714]]), tensor([[ 0.0575,  0.2082, -0.0756, -0.2939]]), tensor([[ 0.0051,  0.0261, -0.0046,  0.0227]]), tensor([[-0.0091, -0.5480,  0.0681,  0.9721]]), tensor([[ 0.0872,  0.6251, -0.1373, -1.1648]]), tensor([[0.0003, 0.0394, 0.0468, 0.0483]]), tensor([[0.0001, 0.0261, 0.0003, 0.0223]]), tensor([[-0.0091, -0.0284,  0.0127, -0.0293]]), tensor([[ 0.0304,  0.2212, -0.0293, -0.2701]]), tensor([[ 0.0374,  0.6206, -0.0501, -1.0585]]), tensor([[-0.0063, -0.0288,  0.0058, -0.0214]]), tensor([[ 0.0353, -0.1681, -0.0345,  0.2947]]), tensor([[ 0.0320,  0.0275, -0.0286, -0.0086]]), tensor([[ 0.0617,  0.0142, -0.0815, -0.0260]]), tensor([[ 0.0213,  0.0438,  0.0054, -0.0491]]), tensor([[ 0.0370,  0.0283, -0.0350, -0.0267]]), tensor([[-0.0314,  0.0140,  0.0464, -0.0220]]), tensor([[ 0.0018,  0.4012,  0.0026, -0.5406]]), tensor([[-0.0189,  0.2072,  0.0285, -0.2725]]), tensor([[ 0.0444, -0.3581, -0.0477,  0.4766]]), tensor([[ 0.0269, -0.1613, -0.0258,  0.1462]]), tensor([[-0.0042,  0.3607,  0.0105, -0.5312]]), tensor([[ 0.0079, -0.1527,  0.0289,  0.2735]]), tensor([[ 0.0220,  0.4161, -0.0182, -0.5570]]), tensor([[-0.0029,  0.2213,  0.0154, -0.2720]])), action=(tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[1]])), next_state=(tensor([[ 0.0170, -0.3469,  0.0134,  0.5450]]), tensor([[ 0.0370,  0.0283, -0.0350, -0.0267]]), tensor([[ 0.0324,  0.7514, -0.0437, -1.1269]]), None, None, tensor([[ 0.1161,  1.0184, -0.1905, -1.8354]]), tensor([[ 0.0872,  0.6251, -0.1373, -1.1648]]), tensor([[-0.0070, -0.0291,  0.0144,  0.0447]]), tensor([[-0.0043,  0.2213,  0.0058, -0.2722]]), tensor([[ 0.0018,  0.4012,  0.0026, -0.5406]]), tensor([[ 0.0079, -0.1527,  0.0289,  0.2735]]), None, tensor([[ 0.0032, -0.0289, -0.0005,  0.0402]]), tensor([[ 0.0213,  0.5899, -0.0606, -0.8856]]), tensor([[ 0.0220,  0.4161, -0.0182, -0.5570]]), tensor([[ 0.0106, -0.1687, -0.0092,  0.3096]]), tensor([[-0.0026,  0.0112,  0.0068,  0.0402]]), tensor([[ 0.0134,  0.3942, -0.0491, -0.5779]]), tensor([[ 0.0662,  0.6222, -0.0986, -1.0970]]), tensor([[ 0.0070,  0.0428,  0.0294, -0.0284]]), tensor([[-0.0041,  0.1662, -0.0012, -0.3109]]), tensor([[-0.0075,  0.1658,  0.0153, -0.2434]]), tensor([[ 0.0326,  0.9475, -0.0615, -1.5019]]), tensor([[-0.0035, -0.0289, -0.0009, -0.0179]]), tensor([[ 0.0374,  0.6206, -0.0501, -1.0585]]), tensor([[ 0.0353, -0.1681, -0.0345,  0.2947]]), tensor([[-0.0062,  0.3934, -0.0206, -0.5612]]), tensor([[-0.0381, -0.7464,  0.1220,  1.3433]]), tensor([[ 0.0064,  0.5566, -0.0195, -0.8990]]), tensor([[ 0.0192,  0.0436,  0.0093, -0.0460]]), tensor([[-0.0314,  0.0140,  0.0464, -0.0220]]), tensor([[ 0.0269, -0.1613, -0.0258,  0.1462]]), tensor([[-0.0176,  0.0271,  0.0229,  0.0005]]), tensor([[-0.0091, -0.5480,  0.0681,  0.9721]]), tensor([[ 0.0662,  0.4066, -0.0889, -0.6606]]), tensor([[-0.0278, -0.1805,  0.0413,  0.2574]]), tensor([[-0.0530, -0.9428,  0.1489,  1.6716]]), tensor([[ 0.0072, -0.3637, -0.0030,  0.5993]]), tensor([[-0.0719, -0.7497,  0.1823,  1.4287]]), tensor([[-0.0171,  0.2219,  0.0229, -0.2848]]), None, tensor([[ 0.0475,  0.9471, -0.0662, -1.4330]]), tensor([[ 0.0201, -0.1516,  0.0084,  0.2496]]), tensor([[ 0.0213,  0.5559, -0.0271, -0.8258]]), tensor([[ 0.0477, -0.1638, -0.0517,  0.2006]]), tensor([[-0.0025, -0.2241,  0.0077,  0.3349]]), tensor([[-0.0042,  0.3607,  0.0105, -0.5312]]), tensor([[ 0.0495,  0.4023, -0.0643, -0.5656]]), tensor([[-0.0024,  0.2062,  0.0076, -0.2503]]), tensor([[0.0003, 0.0394, 0.0468, 0.0483]]), tensor([[-8.2103e-05, -1.6857e-01,  8.9900e-03,  3.0572e-01]]), tensor([[-0.0142, -0.1678,  0.0171,  0.2878]]), tensor([[-0.0019, -0.0289,  0.0069,  0.0401]]), tensor([[ 0.0743,  0.2128, -0.1021, -0.3972]]), tensor([[ 0.0244,  0.2296, -0.0259, -0.4543]]), tensor([[-0.0063, -0.0288,  0.0058, -0.0214]]), tensor([[0.0001, 0.0261, 0.0003, 0.0223]]), tensor([[-0.0008,  0.3613, -0.0075, -0.6039]]), tensor([[-0.0067,  0.2065,  0.0119, -0.2562]]), None, tensor([[-1.7182e-04,  1.6629e-01,  4.5475e-03, -2.5395e-01]]), tensor([[ 0.0471,  0.0306, -0.0502, -0.0758]]), tensor([[ 0.1120,  1.1490, -0.1938, -1.9633]]), tensor([[ 0.0410,  0.5918, -0.0906, -0.9288]]), tensor([[-0.0044,  0.2352,  0.0520, -0.2604]]), tensor([[ 0.1082,  1.1455, -0.1594, -1.8141]]), tensor([[ 0.0778,  0.7558, -0.1354, -1.2932]]), tensor([[ 0.0051,  0.0261, -0.0046,  0.0227]]), tensor([[ 0.0528,  0.7880, -0.1092, -1.2486]]), tensor([[ 0.0893,  0.9490, -0.1298, -1.4838]]), tensor([[-0.0271, -0.5501,  0.1016,  1.0205]]), tensor([[ 0.0098,  0.6112, -0.0012, -0.8493]]), tensor([[ 0.0786,  0.4285, -0.1206, -0.8368]]), tensor([[-0.0035,  0.0264,  0.0151,  0.0159]]), tensor([[ 0.0331,  0.3957, -0.0783, -0.6126]]), tensor([[ 0.0575,  0.2082, -0.0756, -0.2939]]), tensor([[ 0.0289,  0.4251, -0.0350, -0.7550]]), tensor([[-0.0021, -0.1536,  0.0458,  0.2934]]), tensor([[ 0.0340, -0.3569, -0.0348,  0.4496]]), tensor([[ 0.0176,  0.7519, -0.0375, -1.1977]]), tensor([[ 0.0666,  0.5594, -0.1161, -0.9664]]), tensor([[ 0.0217,  0.7916, -0.0249, -1.1278]]), tensor([[ 0.0516,  0.7532, -0.0915, -1.2290]]), tensor([[-0.0102,  0.1981, -0.0154, -0.2638]]), tensor([[ 2.5760e-03, -2.2401e-01,  2.7161e-04,  3.3268e-01]]), tensor([[-0.0126,  0.4167,  0.0172, -0.5702]]), tensor([[ 0.0016,  0.5888, -0.0319, -0.8604]]), tensor([[-0.0020, -0.3522,  0.0549,  0.6627]]), tensor([[-0.0189,  0.2072,  0.0285, -0.2725]]), tensor([[ 0.0101, -0.1519,  0.0243,  0.2565]]), tensor([[-0.0052,  0.0408,  0.0517,  0.0156]]), tensor([[ 0.0929,  0.9524, -0.1613, -1.6250]]), tensor([[ 0.0883,  1.1808, -0.1656, -1.9047]]), tensor([[ 0.0664,  1.1429, -0.0949, -1.7456]]), tensor([[ 0.1119,  0.9878, -0.2037, -1.6676]]), tensor([[-0.0269,  0.4029,  0.0400, -0.5775]]), tensor([[ 0.0420,  0.0294, -0.0421, -0.0490]]), tensor([[ 0.0141,  0.3606, -0.0166, -0.5279]]), tensor([[ 0.0426,  0.2251, -0.0431, -0.3546]]), tensor([[ 0.0444, -0.3581, -0.0477,  0.4766]]), tensor([[ 0.0686,  0.9844, -0.1341, -1.5733]]), tensor([[ 0.1311,  0.9524, -0.1957, -1.5749]]), tensor([[-0.0029,  0.2213,  0.0154, -0.2720]]), tensor([[ 0.0100,  0.0262, -0.0096,  0.0199]]), tensor([[ 0.0617,  0.0142, -0.0815, -0.0260]]), tensor([[ 0.0056,  0.2213, -0.0042, -0.2714]]), tensor([[-0.0200, -0.3539,  0.0876,  0.7016]]), tensor([[ 0.0997,  0.8217, -0.1606, -1.4972]]), tensor([[ 0.0011, -0.1564,  0.0478,  0.3553]]), tensor([[ 0.0007,  0.2212,  0.0008, -0.2702]]), tensor([[-0.0096,  0.1665,  0.0122, -0.3179]]), tensor([[ 0.0348,  0.0265, -0.0347,  0.0132]]), tensor([[ 0.0499,  0.8164, -0.0713, -1.3665]]), tensor([[-0.0069,  0.1663,  0.0054, -0.3123]]), tensor([[ 0.0320,  0.0275, -0.0286, -0.0086]]), tensor([[ 0.0325,  0.2231, -0.0288, -0.3102]]), tensor([[ 0.0620,  0.2104, -0.0820, -0.3432]]), tensor([[ 0.0222, -0.1514,  0.0044,  0.2453]]), tensor([[ 0.0375,  0.2240, -0.0355, -0.3302]]), tensor([[-0.0311,  0.2084,  0.0460, -0.2997]]), tensor([[ 0.0098,  0.5963, -0.0082, -0.8325]]), tensor([[-0.0147,  0.4019,  0.0230, -0.5561]]), tensor([[ 0.0373, -0.1624, -0.0381,  0.1692]]), tensor([[ 0.0237,  0.0341, -0.0228, -0.1545]]), tensor([[ 2.9838e-03,  5.5568e-01, -1.7258e-04, -8.2058e-01]]), tensor([[ 0.0049, -0.3482,  0.0343,  0.5751]]), tensor([[ 0.0304,  0.2212, -0.0293, -0.2701]]), tensor([[ 0.0015,  0.4162,  0.0100, -0.5598]])), reward=(tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.])))\n",
            "[Transition(state=tensor([[ 0.0370,  0.0283, -0.0350, -0.0267]]), action=tensor([[1]]), next_state=tensor([[ 0.0375,  0.2240, -0.0355, -0.3302]]), reward=tensor([1.])), Transition(state=tensor([[-0.0063, -0.0288,  0.0058, -0.0214]]), action=tensor([[1]]), next_state=tensor([[-0.0069,  0.1663,  0.0054, -0.3123]]), reward=tensor([1.])), Transition(state=tensor([[-8.2103e-05, -1.6857e-01,  8.9900e-03,  3.0572e-01]]), action=tensor([[1]]), next_state=tensor([[-0.0035,  0.0264,  0.0151,  0.0159]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0929,  0.9524, -0.1613, -1.6250]]), action=tensor([[1]]), next_state=tensor([[ 0.1120,  1.1490, -0.1938, -1.9633]]), reward=tensor([1.])), Transition(state=tensor([[-0.0126,  0.4167,  0.0172, -0.5702]]), action=tensor([[0]]), next_state=tensor([[-0.0043,  0.2213,  0.0058, -0.2722]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0444, -0.3581, -0.0477,  0.4766]]), action=tensor([[1]]), next_state=tensor([[ 0.0373, -0.1624, -0.0381,  0.1692]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0237,  0.0341, -0.0228, -0.1545]]), action=tensor([[1]]), next_state=tensor([[ 0.0244,  0.2296, -0.0259, -0.4543]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0049, -0.3482,  0.0343,  0.5751]]), action=tensor([[1]]), next_state=tensor([[-0.0021, -0.1536,  0.0458,  0.2934]]), reward=tensor([1.])), Transition(state=tensor([[-1.7182e-04,  1.6629e-01,  4.5475e-03, -2.5395e-01]]), action=tensor([[0]]), next_state=tensor([[ 0.0032, -0.0289, -0.0005,  0.0402]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0289,  0.4251, -0.0350, -0.7550]]), action=tensor([[1]]), next_state=tensor([[ 0.0374,  0.6206, -0.0501, -1.0585]]), reward=tensor([1.])), Transition(state=tensor([[-0.0189,  0.2072,  0.0285, -0.2725]]), action=tensor([[1]]), next_state=tensor([[-0.0147,  0.4019,  0.0230, -0.5561]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0375,  0.2240, -0.0355, -0.3302]]), action=tensor([[0]]), next_state=tensor([[ 0.0420,  0.0294, -0.0421, -0.0490]]), reward=tensor([1.])), Transition(state=tensor([[-0.0044,  0.2352,  0.0520, -0.2604]]), action=tensor([[0]]), next_state=tensor([[0.0003, 0.0394, 0.0468, 0.0483]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0320,  0.0275, -0.0286, -0.0086]]), action=tensor([[1]]), next_state=tensor([[ 0.0325,  0.2231, -0.0288, -0.3102]]), reward=tensor([1.])), Transition(state=tensor([[-0.0147,  0.4019,  0.0230, -0.5561]]), action=tensor([[0]]), next_state=tensor([[-0.0067,  0.2065,  0.0119, -0.2562]]), reward=tensor([1.])), Transition(state=tensor([[-0.0530, -0.9428,  0.1489,  1.6716]]), action=tensor([[1]]), next_state=tensor([[-0.0719, -0.7497,  0.1823,  1.4287]]), reward=tensor([1.])), Transition(state=tensor([[ 2.9838e-03,  5.5568e-01, -1.7258e-04, -8.2058e-01]]), action=tensor([[0]]), next_state=tensor([[ 0.0141,  0.3606, -0.0166, -0.5279]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0872,  0.6251, -0.1373, -1.1648]]), action=tensor([[1]]), next_state=tensor([[ 0.0997,  0.8217, -0.1606, -1.4972]]), reward=tensor([1.])), Transition(state=tensor([[-0.0062,  0.3934, -0.0206, -0.5612]]), action=tensor([[1]]), next_state=tensor([[ 0.0016,  0.5888, -0.0319, -0.8604]]), reward=tensor([1.])), Transition(state=tensor([[-0.0026,  0.0112,  0.0068,  0.0402]]), action=tensor([[1]]), next_state=tensor([[-0.0024,  0.2062,  0.0076, -0.2503]]), reward=tensor([1.])), Transition(state=tensor([[-0.0067,  0.2065,  0.0119, -0.2562]]), action=tensor([[0]]), next_state=tensor([[-0.0026,  0.0112,  0.0068,  0.0402]]), reward=tensor([1.])), Transition(state=tensor([[-0.0075,  0.1658,  0.0153, -0.2434]]), action=tensor([[1]]), next_state=tensor([[-0.0042,  0.3607,  0.0105, -0.5312]]), reward=tensor([1.])), Transition(state=tensor([[-0.0035,  0.0264,  0.0151,  0.0159]]), action=tensor([[1]]), next_state=tensor([[-0.0029,  0.2213,  0.0154, -0.2720]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0374,  0.6206, -0.0501, -1.0585]]), action=tensor([[1]]), next_state=tensor([[ 0.0499,  0.8164, -0.0713, -1.3665]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0213,  0.0438,  0.0054, -0.0491]]), action=tensor([[0]]), next_state=tensor([[ 0.0222, -0.1514,  0.0044,  0.2453]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0499,  0.8164, -0.0713, -1.3665]]), action=tensor([[0]]), next_state=tensor([[ 0.0662,  0.6222, -0.0986, -1.0970]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0170, -0.3469,  0.0134,  0.5450]]), action=tensor([[1]]), next_state=tensor([[ 0.0101, -0.1519,  0.0243,  0.2565]]), reward=tensor([1.])), Transition(state=tensor([[-0.0171,  0.2219,  0.0229, -0.2848]]), action=tensor([[1]]), next_state=tensor([[-0.0126,  0.4167,  0.0172, -0.5702]]), reward=tensor([1.])), Transition(state=tensor([[-0.0042,  0.3607,  0.0105, -0.5312]]), action=tensor([[1]]), next_state=tensor([[ 2.9838e-03,  5.5568e-01, -1.7258e-04, -8.2058e-01]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0134,  0.3942, -0.0491, -0.5779]]), action=tensor([[1]]), next_state=tensor([[ 0.0213,  0.5899, -0.0606, -0.8856]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0032, -0.0289, -0.0005,  0.0402]]), action=tensor([[0]]), next_state=tensor([[ 2.5760e-03, -2.2401e-01,  2.7161e-04,  3.3268e-01]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0304,  0.2212, -0.0293, -0.2701]]), action=tensor([[0]]), next_state=tensor([[ 0.0348,  0.0265, -0.0347,  0.0132]]), reward=tensor([1.])), Transition(state=tensor([[-0.0102,  0.1981, -0.0154, -0.2638]]), action=tensor([[1]]), next_state=tensor([[-0.0062,  0.3934, -0.0206, -0.5612]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0141,  0.3606, -0.0166, -0.5279]]), action=tensor([[1]]), next_state=tensor([[ 0.0213,  0.5559, -0.0271, -0.8258]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0477, -0.1638, -0.0517,  0.2006]]), action=tensor([[0]]), next_state=tensor([[ 0.0444, -0.3581, -0.0477,  0.4766]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0617,  0.0142, -0.0815, -0.0260]]), action=tensor([[1]]), next_state=tensor([[ 0.0620,  0.2104, -0.0820, -0.3432]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0011, -0.1564,  0.0478,  0.3553]]), action=tensor([[0]]), next_state=tensor([[-0.0020, -0.3522,  0.0549,  0.6627]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0686,  0.9844, -0.1341, -1.5733]]), action=tensor([[1]]), next_state=tensor([[ 0.0883,  1.1808, -0.1656, -1.9047]]), reward=tensor([1.])), Transition(state=tensor([[-0.0091, -0.5480,  0.0681,  0.9721]]), action=tensor([[1]]), next_state=tensor([[-0.0200, -0.3539,  0.0876,  0.7016]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0015,  0.4162,  0.0100, -0.5598]]), action=tensor([[1]]), next_state=tensor([[ 0.0098,  0.6112, -0.0012, -0.8493]]), reward=tensor([1.])), Transition(state=tensor([[-0.0020, -0.3522,  0.0549,  0.6627]]), action=tensor([[0]]), next_state=tensor([[-0.0091, -0.5480,  0.0681,  0.9721]]), reward=tensor([1.])), Transition(state=tensor([[-0.0311,  0.2084,  0.0460, -0.2997]]), action=tensor([[1]]), next_state=tensor([[-0.0269,  0.4029,  0.0400, -0.5775]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0217,  0.7916, -0.0249, -1.1278]]), action=tensor([[0]]), next_state=tensor([[ 0.0376,  0.5968, -0.0475, -0.8430]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0664,  1.1429, -0.0949, -1.7456]]), action=tensor([[0]]), next_state=tensor([[ 0.0893,  0.9490, -0.1298, -1.4838]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0326,  0.9475, -0.0615, -1.5019]]), action=tensor([[0]]), next_state=tensor([[ 0.0516,  0.7532, -0.0915, -1.2290]]), reward=tensor([1.])), Transition(state=tensor([[-0.0035, -0.0289, -0.0009, -0.0179]]), action=tensor([[1]]), next_state=tensor([[-0.0041,  0.1662, -0.0012, -0.3109]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0325,  0.2231, -0.0288, -0.3102]]), action=tensor([[0]]), next_state=tensor([[ 0.0370,  0.0283, -0.0350, -0.0267]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0056,  0.2213, -0.0042, -0.2714]]), action=tensor([[0]]), next_state=tensor([[ 0.0100,  0.0262, -0.0096,  0.0199]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0662,  0.6222, -0.0986, -1.0970]]), action=tensor([[0]]), next_state=tensor([[ 0.0786,  0.4285, -0.1206, -0.8368]]), reward=tensor([1.])), Transition(state=tensor([[ 0.1161,  1.0184, -0.1905, -1.8354]]), action=tensor([[1]]), next_state=None, reward=tensor([1.])), Transition(state=tensor([[-0.0041,  0.1662, -0.0012, -0.3109]]), action=tensor([[1]]), next_state=tensor([[-0.0008,  0.3613, -0.0075, -0.6039]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0018,  0.4012,  0.0026, -0.5406]]), action=tensor([[1]]), next_state=tensor([[ 0.0098,  0.5963, -0.0082, -0.8325]]), reward=tensor([1.])), Transition(state=tensor([[-0.0381, -0.7464,  0.1220,  1.3433]]), action=tensor([[0]]), next_state=tensor([[-0.0530, -0.9428,  0.1489,  1.6716]]), reward=tensor([1.])), Transition(state=tensor([[-0.0096,  0.1665,  0.0122, -0.3179]]), action=tensor([[0]]), next_state=tensor([[-0.0063, -0.0288,  0.0058, -0.0214]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0495,  0.4023, -0.0643, -0.5656]]), action=tensor([[0]]), next_state=tensor([[ 0.0575,  0.2082, -0.0756, -0.2939]]), reward=tensor([1.])), Transition(state=tensor([[-0.0052,  0.0408,  0.0517,  0.0156]]), action=tensor([[1]]), next_state=tensor([[-0.0044,  0.2352,  0.0520, -0.2604]]), reward=tensor([1.])), Transition(state=tensor([[0.0001, 0.0261, 0.0003, 0.0223]]), action=tensor([[1]]), next_state=tensor([[ 0.0007,  0.2212,  0.0008, -0.2702]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0098,  0.6112, -0.0012, -0.8493]]), action=tensor([[0]]), next_state=tensor([[ 0.0220,  0.4161, -0.0182, -0.5570]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0516,  0.7532, -0.0915, -1.2290]]), action=tensor([[0]]), next_state=tensor([[ 0.0666,  0.5594, -0.1161, -0.9664]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0106, -0.1687, -0.0092,  0.3096]]), action=tensor([[0]]), next_state=tensor([[ 0.0072, -0.3637, -0.0030,  0.5993]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0192,  0.0436,  0.0093, -0.0460]]), action=tensor([[0]]), next_state=tensor([[ 0.0201, -0.1516,  0.0084,  0.2496]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0101, -0.1519,  0.0243,  0.2565]]), action=tensor([[1]]), next_state=tensor([[ 0.0070,  0.0428,  0.0294, -0.0284]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0786,  0.4285, -0.1206, -0.8368]]), action=tensor([[1]]), next_state=tensor([[ 0.0872,  0.6251, -0.1373, -1.1648]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0471,  0.0306, -0.0502, -0.0758]]), action=tensor([[0]]), next_state=tensor([[ 0.0477, -0.1638, -0.0517,  0.2006]]), reward=tensor([1.])), Transition(state=tensor([[-0.0719, -0.7497,  0.1823,  1.4287]]), action=tensor([[0]]), next_state=None, reward=tensor([1.])), Transition(state=tensor([[ 0.0201, -0.1516,  0.0084,  0.2496]]), action=tensor([[0]]), next_state=tensor([[ 0.0170, -0.3469,  0.0134,  0.5450]]), reward=tensor([1.])), Transition(state=tensor([[-0.0024,  0.2062,  0.0076, -0.2503]]), action=tensor([[1]]), next_state=tensor([[ 0.0018,  0.4012,  0.0026, -0.5406]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0475,  0.9471, -0.0662, -1.4330]]), action=tensor([[1]]), next_state=tensor([[ 0.0664,  1.1429, -0.0949, -1.7456]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0213,  0.5559, -0.0271, -0.8258]]), action=tensor([[1]]), next_state=tensor([[ 0.0324,  0.7514, -0.0437, -1.1269]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0376,  0.5968, -0.0475, -0.8430]]), action=tensor([[0]]), next_state=tensor([[ 0.0495,  0.4023, -0.0643, -0.5656]]), reward=tensor([1.])), Transition(state=tensor([[ 0.1120,  1.1490, -0.1938, -1.9633]]), action=tensor([[1]]), next_state=None, reward=tensor([1.])), Transition(state=tensor([[-0.0069,  0.1663,  0.0054, -0.3123]]), action=tensor([[0]]), next_state=tensor([[-0.0035, -0.0289, -0.0009, -0.0179]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0743,  0.2128, -0.1021, -0.3972]]), action=tensor([[1]]), next_state=tensor([[ 0.0786,  0.4092, -0.1100, -0.7202]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0072, -0.3637, -0.0030,  0.5993]]), action=tensor([[1]]), next_state=tensor([[-8.2103e-05, -1.6857e-01,  8.9900e-03,  3.0572e-01]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0666,  0.5594, -0.1161, -0.9664]]), action=tensor([[1]]), next_state=tensor([[ 0.0778,  0.7558, -0.1354, -1.2932]]), reward=tensor([1.])), Transition(state=tensor([[-0.0070, -0.0291,  0.0144,  0.0447]]), action=tensor([[1]]), next_state=tensor([[-0.0075,  0.1658,  0.0153, -0.2434]]), reward=tensor([1.])), Transition(state=tensor([[ 0.1119,  0.9878, -0.2037, -1.6676]]), action=tensor([[0]]), next_state=None, reward=tensor([1.])), Transition(state=tensor([[ 0.0997,  0.8217, -0.1606, -1.4972]]), action=tensor([[1]]), next_state=tensor([[ 0.1161,  1.0184, -0.1905, -1.8354]]), reward=tensor([1.])), Transition(state=tensor([[ 0.1082,  1.1455, -0.1594, -1.8141]]), action=tensor([[0]]), next_state=tensor([[ 0.1311,  0.9524, -0.1957, -1.5749]]), reward=tensor([1.])), Transition(state=tensor([[-0.0148,  0.0276,  0.0173, -0.0103]]), action=tensor([[0]]), next_state=tensor([[-0.0142, -0.1678,  0.0171,  0.2878]]), reward=tensor([1.])), Transition(state=tensor([[ 0.1311,  0.9524, -0.1957, -1.5749]]), action=tensor([[0]]), next_state=None, reward=tensor([1.])), Transition(state=tensor([[ 0.0220,  0.4161, -0.0182, -0.5570]]), action=tensor([[0]]), next_state=tensor([[ 0.0304,  0.2212, -0.0293, -0.2701]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0353, -0.1681, -0.0345,  0.2947]]), action=tensor([[1]]), next_state=tensor([[ 0.0320,  0.0275, -0.0286, -0.0086]]), reward=tensor([1.])), Transition(state=tensor([[-0.0314,  0.0140,  0.0464, -0.0220]]), action=tensor([[1]]), next_state=tensor([[-0.0311,  0.2084,  0.0460, -0.2997]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0176,  0.7519, -0.0375, -1.1977]]), action=tensor([[1]]), next_state=tensor([[ 0.0326,  0.9475, -0.0615, -1.5019]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0269, -0.1613, -0.0258,  0.1462]]), action=tensor([[1]]), next_state=tensor([[ 0.0237,  0.0341, -0.0228, -0.1545]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0620,  0.2104, -0.0820, -0.3432]]), action=tensor([[1]]), next_state=tensor([[ 0.0662,  0.4066, -0.0889, -0.6606]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0098,  0.5963, -0.0082, -0.8325]]), action=tensor([[1]]), next_state=tensor([[ 0.0217,  0.7916, -0.0249, -1.1278]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0079, -0.1527,  0.0289,  0.2735]]), action=tensor([[0]]), next_state=tensor([[ 0.0049, -0.3482,  0.0343,  0.5751]]), reward=tensor([1.])), Transition(state=tensor([[0.0003, 0.0394, 0.0468, 0.0483]]), action=tensor([[0]]), next_state=tensor([[ 0.0011, -0.1564,  0.0478,  0.3553]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0100,  0.0262, -0.0096,  0.0199]]), action=tensor([[0]]), next_state=tensor([[ 0.0106, -0.1687, -0.0092,  0.3096]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0420,  0.0294, -0.0421, -0.0490]]), action=tensor([[1]]), next_state=tensor([[ 0.0426,  0.2251, -0.0431, -0.3546]]), reward=tensor([1.])), Transition(state=tensor([[-0.0271, -0.5501,  0.1016,  1.0205]]), action=tensor([[0]]), next_state=tensor([[-0.0381, -0.7464,  0.1220,  1.3433]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0373, -0.1624, -0.0381,  0.1692]]), action=tensor([[0]]), next_state=tensor([[ 0.0340, -0.3569, -0.0348,  0.4496]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0340, -0.3569, -0.0348,  0.4496]]), action=tensor([[1]]), next_state=tensor([[ 0.0269, -0.1613, -0.0258,  0.1462]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0331,  0.3957, -0.0783, -0.6126]]), action=tensor([[1]]), next_state=tensor([[ 0.0410,  0.5918, -0.0906, -0.9288]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0004, -0.0288,  0.0038,  0.0375]]), action=tensor([[1]]), next_state=tensor([[-1.7182e-04,  1.6629e-01,  4.5475e-03, -2.5395e-01]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0426,  0.2251, -0.0431, -0.3546]]), action=tensor([[0]]), next_state=tensor([[ 0.0471,  0.0306, -0.0502, -0.0758]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0575,  0.2082, -0.0756, -0.2939]]), action=tensor([[0]]), next_state=tensor([[ 0.0617,  0.0142, -0.0815, -0.0260]]), reward=tensor([1.])), Transition(state=tensor([[-0.0025, -0.2241,  0.0077,  0.3349]]), action=tensor([[1]]), next_state=tensor([[-0.0070, -0.0291,  0.0144,  0.0447]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0410,  0.5918, -0.0906, -0.9288]]), action=tensor([[1]]), next_state=tensor([[ 0.0528,  0.7880, -0.1092, -1.2486]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0893,  0.9490, -0.1298, -1.4838]]), action=tensor([[1]]), next_state=tensor([[ 0.1082,  1.1455, -0.1594, -1.8141]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0222, -0.1514,  0.0044,  0.2453]]), action=tensor([[1]]), next_state=tensor([[ 0.0192,  0.0436,  0.0093, -0.0460]]), reward=tensor([1.])), Transition(state=tensor([[-0.0019, -0.0289,  0.0069,  0.0401]]), action=tensor([[0]]), next_state=tensor([[-0.0025, -0.2241,  0.0077,  0.3349]]), reward=tensor([1.])), Transition(state=tensor([[-0.0142, -0.1678,  0.0171,  0.2878]]), action=tensor([[1]]), next_state=tensor([[-0.0176,  0.0271,  0.0229,  0.0005]]), reward=tensor([1.])), Transition(state=tensor([[-0.0278, -0.1805,  0.0413,  0.2574]]), action=tensor([[1]]), next_state=tensor([[-0.0314,  0.0140,  0.0464, -0.0220]]), reward=tensor([1.])), Transition(state=tensor([[-0.0021, -0.1536,  0.0458,  0.2934]]), action=tensor([[1]]), next_state=tensor([[-0.0052,  0.0408,  0.0517,  0.0156]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0007,  0.2212,  0.0008, -0.2702]]), action=tensor([[0]]), next_state=tensor([[ 0.0051,  0.0261, -0.0046,  0.0227]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0213,  0.5899, -0.0606, -0.8856]]), action=tensor([[0]]), next_state=tensor([[ 0.0331,  0.3957, -0.0783, -0.6126]]), reward=tensor([1.])), Transition(state=tensor([[-0.0269,  0.4029,  0.0400, -0.5775]]), action=tensor([[0]]), next_state=tensor([[-0.0189,  0.2072,  0.0285, -0.2725]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0662,  0.4066, -0.0889, -0.6606]]), action=tensor([[0]]), next_state=tensor([[ 0.0743,  0.2128, -0.1021, -0.3972]]), reward=tensor([1.])), Transition(state=tensor([[-0.0102,  0.0027, -0.0160,  0.0339]]), action=tensor([[1]]), next_state=tensor([[-0.0102,  0.1981, -0.0154, -0.2638]]), reward=tensor([1.])), Transition(state=tensor([[ 2.5760e-03, -2.2401e-01,  2.7161e-04,  3.3268e-01]]), action=tensor([[1]]), next_state=tensor([[-0.0019, -0.0289,  0.0069,  0.0401]]), reward=tensor([1.])), Transition(state=tensor([[-0.0281,  0.0152,  0.0423, -0.0483]]), action=tensor([[0]]), next_state=tensor([[-0.0278, -0.1805,  0.0413,  0.2574]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0070,  0.0428,  0.0294, -0.0284]]), action=tensor([[0]]), next_state=tensor([[ 0.0079, -0.1527,  0.0289,  0.2735]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0778,  0.7558, -0.1354, -1.2932]]), action=tensor([[1]]), next_state=tensor([[ 0.0929,  0.9524, -0.1613, -1.6250]]), reward=tensor([1.])), Transition(state=tensor([[-0.0176,  0.0271,  0.0229,  0.0005]]), action=tensor([[1]]), next_state=tensor([[-0.0171,  0.2219,  0.0229, -0.2848]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0051,  0.0261, -0.0046,  0.0227]]), action=tensor([[1]]), next_state=tensor([[ 0.0056,  0.2213, -0.0042, -0.2714]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0883,  1.1808, -0.1656, -1.9047]]), action=tensor([[0]]), next_state=tensor([[ 0.1119,  0.9878, -0.2037, -1.6676]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0324,  0.7514, -0.0437, -1.1269]]), action=tensor([[1]]), next_state=tensor([[ 0.0475,  0.9471, -0.0662, -1.4330]]), reward=tensor([1.])), Transition(state=tensor([[-0.0008,  0.3613, -0.0075, -0.6039]]), action=tensor([[1]]), next_state=tensor([[ 0.0064,  0.5566, -0.0195, -0.8990]]), reward=tensor([1.])), Transition(state=tensor([[-0.0043,  0.2213,  0.0058, -0.2722]]), action=tensor([[0]]), next_state=tensor([[0.0001, 0.0261, 0.0003, 0.0223]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0064,  0.5566, -0.0195, -0.8990]]), action=tensor([[1]]), next_state=tensor([[ 0.0176,  0.7519, -0.0375, -1.1977]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0348,  0.0265, -0.0347,  0.0132]]), action=tensor([[0]]), next_state=tensor([[ 0.0353, -0.1681, -0.0345,  0.2947]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0016,  0.5888, -0.0319, -0.8604]]), action=tensor([[0]]), next_state=tensor([[ 0.0134,  0.3942, -0.0491, -0.5779]]), reward=tensor([1.])), Transition(state=tensor([[-0.0200, -0.3539,  0.0876,  0.7016]]), action=tensor([[0]]), next_state=tensor([[-0.0271, -0.5501,  0.1016,  1.0205]]), reward=tensor([1.])), Transition(state=tensor([[-0.0091, -0.0284,  0.0127, -0.0293]]), action=tensor([[1]]), next_state=tensor([[-0.0096,  0.1665,  0.0122, -0.3179]]), reward=tensor([1.])), Transition(state=tensor([[ 0.0244,  0.2296, -0.0259, -0.4543]]), action=tensor([[1]]), next_state=tensor([[ 0.0289,  0.4251, -0.0350, -0.7550]]), reward=tensor([1.]))]\n",
            "Transition(state=(tensor([[ 0.0370,  0.0283, -0.0350, -0.0267]]), tensor([[-0.0063, -0.0288,  0.0058, -0.0214]]), tensor([[-8.2103e-05, -1.6857e-01,  8.9900e-03,  3.0572e-01]]), tensor([[ 0.0929,  0.9524, -0.1613, -1.6250]]), tensor([[-0.0126,  0.4167,  0.0172, -0.5702]]), tensor([[ 0.0444, -0.3581, -0.0477,  0.4766]]), tensor([[ 0.0237,  0.0341, -0.0228, -0.1545]]), tensor([[ 0.0049, -0.3482,  0.0343,  0.5751]]), tensor([[-1.7182e-04,  1.6629e-01,  4.5475e-03, -2.5395e-01]]), tensor([[ 0.0289,  0.4251, -0.0350, -0.7550]]), tensor([[-0.0189,  0.2072,  0.0285, -0.2725]]), tensor([[ 0.0375,  0.2240, -0.0355, -0.3302]]), tensor([[-0.0044,  0.2352,  0.0520, -0.2604]]), tensor([[ 0.0320,  0.0275, -0.0286, -0.0086]]), tensor([[-0.0147,  0.4019,  0.0230, -0.5561]]), tensor([[-0.0530, -0.9428,  0.1489,  1.6716]]), tensor([[ 2.9838e-03,  5.5568e-01, -1.7258e-04, -8.2058e-01]]), tensor([[ 0.0872,  0.6251, -0.1373, -1.1648]]), tensor([[-0.0062,  0.3934, -0.0206, -0.5612]]), tensor([[-0.0026,  0.0112,  0.0068,  0.0402]]), tensor([[-0.0067,  0.2065,  0.0119, -0.2562]]), tensor([[-0.0075,  0.1658,  0.0153, -0.2434]]), tensor([[-0.0035,  0.0264,  0.0151,  0.0159]]), tensor([[ 0.0374,  0.6206, -0.0501, -1.0585]]), tensor([[ 0.0213,  0.0438,  0.0054, -0.0491]]), tensor([[ 0.0499,  0.8164, -0.0713, -1.3665]]), tensor([[ 0.0170, -0.3469,  0.0134,  0.5450]]), tensor([[-0.0171,  0.2219,  0.0229, -0.2848]]), tensor([[-0.0042,  0.3607,  0.0105, -0.5312]]), tensor([[ 0.0134,  0.3942, -0.0491, -0.5779]]), tensor([[ 0.0032, -0.0289, -0.0005,  0.0402]]), tensor([[ 0.0304,  0.2212, -0.0293, -0.2701]]), tensor([[-0.0102,  0.1981, -0.0154, -0.2638]]), tensor([[ 0.0141,  0.3606, -0.0166, -0.5279]]), tensor([[ 0.0477, -0.1638, -0.0517,  0.2006]]), tensor([[ 0.0617,  0.0142, -0.0815, -0.0260]]), tensor([[ 0.0011, -0.1564,  0.0478,  0.3553]]), tensor([[ 0.0686,  0.9844, -0.1341, -1.5733]]), tensor([[-0.0091, -0.5480,  0.0681,  0.9721]]), tensor([[ 0.0015,  0.4162,  0.0100, -0.5598]]), tensor([[-0.0020, -0.3522,  0.0549,  0.6627]]), tensor([[-0.0311,  0.2084,  0.0460, -0.2997]]), tensor([[ 0.0217,  0.7916, -0.0249, -1.1278]]), tensor([[ 0.0664,  1.1429, -0.0949, -1.7456]]), tensor([[ 0.0326,  0.9475, -0.0615, -1.5019]]), tensor([[-0.0035, -0.0289, -0.0009, -0.0179]]), tensor([[ 0.0325,  0.2231, -0.0288, -0.3102]]), tensor([[ 0.0056,  0.2213, -0.0042, -0.2714]]), tensor([[ 0.0662,  0.6222, -0.0986, -1.0970]]), tensor([[ 0.1161,  1.0184, -0.1905, -1.8354]]), tensor([[-0.0041,  0.1662, -0.0012, -0.3109]]), tensor([[ 0.0018,  0.4012,  0.0026, -0.5406]]), tensor([[-0.0381, -0.7464,  0.1220,  1.3433]]), tensor([[-0.0096,  0.1665,  0.0122, -0.3179]]), tensor([[ 0.0495,  0.4023, -0.0643, -0.5656]]), tensor([[-0.0052,  0.0408,  0.0517,  0.0156]]), tensor([[0.0001, 0.0261, 0.0003, 0.0223]]), tensor([[ 0.0098,  0.6112, -0.0012, -0.8493]]), tensor([[ 0.0516,  0.7532, -0.0915, -1.2290]]), tensor([[ 0.0106, -0.1687, -0.0092,  0.3096]]), tensor([[ 0.0192,  0.0436,  0.0093, -0.0460]]), tensor([[ 0.0101, -0.1519,  0.0243,  0.2565]]), tensor([[ 0.0786,  0.4285, -0.1206, -0.8368]]), tensor([[ 0.0471,  0.0306, -0.0502, -0.0758]]), tensor([[-0.0719, -0.7497,  0.1823,  1.4287]]), tensor([[ 0.0201, -0.1516,  0.0084,  0.2496]]), tensor([[-0.0024,  0.2062,  0.0076, -0.2503]]), tensor([[ 0.0475,  0.9471, -0.0662, -1.4330]]), tensor([[ 0.0213,  0.5559, -0.0271, -0.8258]]), tensor([[ 0.0376,  0.5968, -0.0475, -0.8430]]), tensor([[ 0.1120,  1.1490, -0.1938, -1.9633]]), tensor([[-0.0069,  0.1663,  0.0054, -0.3123]]), tensor([[ 0.0743,  0.2128, -0.1021, -0.3972]]), tensor([[ 0.0072, -0.3637, -0.0030,  0.5993]]), tensor([[ 0.0666,  0.5594, -0.1161, -0.9664]]), tensor([[-0.0070, -0.0291,  0.0144,  0.0447]]), tensor([[ 0.1119,  0.9878, -0.2037, -1.6676]]), tensor([[ 0.0997,  0.8217, -0.1606, -1.4972]]), tensor([[ 0.1082,  1.1455, -0.1594, -1.8141]]), tensor([[-0.0148,  0.0276,  0.0173, -0.0103]]), tensor([[ 0.1311,  0.9524, -0.1957, -1.5749]]), tensor([[ 0.0220,  0.4161, -0.0182, -0.5570]]), tensor([[ 0.0353, -0.1681, -0.0345,  0.2947]]), tensor([[-0.0314,  0.0140,  0.0464, -0.0220]]), tensor([[ 0.0176,  0.7519, -0.0375, -1.1977]]), tensor([[ 0.0269, -0.1613, -0.0258,  0.1462]]), tensor([[ 0.0620,  0.2104, -0.0820, -0.3432]]), tensor([[ 0.0098,  0.5963, -0.0082, -0.8325]]), tensor([[ 0.0079, -0.1527,  0.0289,  0.2735]]), tensor([[0.0003, 0.0394, 0.0468, 0.0483]]), tensor([[ 0.0100,  0.0262, -0.0096,  0.0199]]), tensor([[ 0.0420,  0.0294, -0.0421, -0.0490]]), tensor([[-0.0271, -0.5501,  0.1016,  1.0205]]), tensor([[ 0.0373, -0.1624, -0.0381,  0.1692]]), tensor([[ 0.0340, -0.3569, -0.0348,  0.4496]]), tensor([[ 0.0331,  0.3957, -0.0783, -0.6126]]), tensor([[ 0.0004, -0.0288,  0.0038,  0.0375]]), tensor([[ 0.0426,  0.2251, -0.0431, -0.3546]]), tensor([[ 0.0575,  0.2082, -0.0756, -0.2939]]), tensor([[-0.0025, -0.2241,  0.0077,  0.3349]]), tensor([[ 0.0410,  0.5918, -0.0906, -0.9288]]), tensor([[ 0.0893,  0.9490, -0.1298, -1.4838]]), tensor([[ 0.0222, -0.1514,  0.0044,  0.2453]]), tensor([[-0.0019, -0.0289,  0.0069,  0.0401]]), tensor([[-0.0142, -0.1678,  0.0171,  0.2878]]), tensor([[-0.0278, -0.1805,  0.0413,  0.2574]]), tensor([[-0.0021, -0.1536,  0.0458,  0.2934]]), tensor([[ 0.0007,  0.2212,  0.0008, -0.2702]]), tensor([[ 0.0213,  0.5899, -0.0606, -0.8856]]), tensor([[-0.0269,  0.4029,  0.0400, -0.5775]]), tensor([[ 0.0662,  0.4066, -0.0889, -0.6606]]), tensor([[-0.0102,  0.0027, -0.0160,  0.0339]]), tensor([[ 2.5760e-03, -2.2401e-01,  2.7161e-04,  3.3268e-01]]), tensor([[-0.0281,  0.0152,  0.0423, -0.0483]]), tensor([[ 0.0070,  0.0428,  0.0294, -0.0284]]), tensor([[ 0.0778,  0.7558, -0.1354, -1.2932]]), tensor([[-0.0176,  0.0271,  0.0229,  0.0005]]), tensor([[ 0.0051,  0.0261, -0.0046,  0.0227]]), tensor([[ 0.0883,  1.1808, -0.1656, -1.9047]]), tensor([[ 0.0324,  0.7514, -0.0437, -1.1269]]), tensor([[-0.0008,  0.3613, -0.0075, -0.6039]]), tensor([[-0.0043,  0.2213,  0.0058, -0.2722]]), tensor([[ 0.0064,  0.5566, -0.0195, -0.8990]]), tensor([[ 0.0348,  0.0265, -0.0347,  0.0132]]), tensor([[ 0.0016,  0.5888, -0.0319, -0.8604]]), tensor([[-0.0200, -0.3539,  0.0876,  0.7016]]), tensor([[-0.0091, -0.0284,  0.0127, -0.0293]]), tensor([[ 0.0244,  0.2296, -0.0259, -0.4543]])), action=(tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[1]]), tensor([[0]]), tensor([[1]]), tensor([[0]]), tensor([[0]]), tensor([[0]]), tensor([[1]]), tensor([[1]])), next_state=(tensor([[ 0.0375,  0.2240, -0.0355, -0.3302]]), tensor([[-0.0069,  0.1663,  0.0054, -0.3123]]), tensor([[-0.0035,  0.0264,  0.0151,  0.0159]]), tensor([[ 0.1120,  1.1490, -0.1938, -1.9633]]), tensor([[-0.0043,  0.2213,  0.0058, -0.2722]]), tensor([[ 0.0373, -0.1624, -0.0381,  0.1692]]), tensor([[ 0.0244,  0.2296, -0.0259, -0.4543]]), tensor([[-0.0021, -0.1536,  0.0458,  0.2934]]), tensor([[ 0.0032, -0.0289, -0.0005,  0.0402]]), tensor([[ 0.0374,  0.6206, -0.0501, -1.0585]]), tensor([[-0.0147,  0.4019,  0.0230, -0.5561]]), tensor([[ 0.0420,  0.0294, -0.0421, -0.0490]]), tensor([[0.0003, 0.0394, 0.0468, 0.0483]]), tensor([[ 0.0325,  0.2231, -0.0288, -0.3102]]), tensor([[-0.0067,  0.2065,  0.0119, -0.2562]]), tensor([[-0.0719, -0.7497,  0.1823,  1.4287]]), tensor([[ 0.0141,  0.3606, -0.0166, -0.5279]]), tensor([[ 0.0997,  0.8217, -0.1606, -1.4972]]), tensor([[ 0.0016,  0.5888, -0.0319, -0.8604]]), tensor([[-0.0024,  0.2062,  0.0076, -0.2503]]), tensor([[-0.0026,  0.0112,  0.0068,  0.0402]]), tensor([[-0.0042,  0.3607,  0.0105, -0.5312]]), tensor([[-0.0029,  0.2213,  0.0154, -0.2720]]), tensor([[ 0.0499,  0.8164, -0.0713, -1.3665]]), tensor([[ 0.0222, -0.1514,  0.0044,  0.2453]]), tensor([[ 0.0662,  0.6222, -0.0986, -1.0970]]), tensor([[ 0.0101, -0.1519,  0.0243,  0.2565]]), tensor([[-0.0126,  0.4167,  0.0172, -0.5702]]), tensor([[ 2.9838e-03,  5.5568e-01, -1.7258e-04, -8.2058e-01]]), tensor([[ 0.0213,  0.5899, -0.0606, -0.8856]]), tensor([[ 2.5760e-03, -2.2401e-01,  2.7161e-04,  3.3268e-01]]), tensor([[ 0.0348,  0.0265, -0.0347,  0.0132]]), tensor([[-0.0062,  0.3934, -0.0206, -0.5612]]), tensor([[ 0.0213,  0.5559, -0.0271, -0.8258]]), tensor([[ 0.0444, -0.3581, -0.0477,  0.4766]]), tensor([[ 0.0620,  0.2104, -0.0820, -0.3432]]), tensor([[-0.0020, -0.3522,  0.0549,  0.6627]]), tensor([[ 0.0883,  1.1808, -0.1656, -1.9047]]), tensor([[-0.0200, -0.3539,  0.0876,  0.7016]]), tensor([[ 0.0098,  0.6112, -0.0012, -0.8493]]), tensor([[-0.0091, -0.5480,  0.0681,  0.9721]]), tensor([[-0.0269,  0.4029,  0.0400, -0.5775]]), tensor([[ 0.0376,  0.5968, -0.0475, -0.8430]]), tensor([[ 0.0893,  0.9490, -0.1298, -1.4838]]), tensor([[ 0.0516,  0.7532, -0.0915, -1.2290]]), tensor([[-0.0041,  0.1662, -0.0012, -0.3109]]), tensor([[ 0.0370,  0.0283, -0.0350, -0.0267]]), tensor([[ 0.0100,  0.0262, -0.0096,  0.0199]]), tensor([[ 0.0786,  0.4285, -0.1206, -0.8368]]), None, tensor([[-0.0008,  0.3613, -0.0075, -0.6039]]), tensor([[ 0.0098,  0.5963, -0.0082, -0.8325]]), tensor([[-0.0530, -0.9428,  0.1489,  1.6716]]), tensor([[-0.0063, -0.0288,  0.0058, -0.0214]]), tensor([[ 0.0575,  0.2082, -0.0756, -0.2939]]), tensor([[-0.0044,  0.2352,  0.0520, -0.2604]]), tensor([[ 0.0007,  0.2212,  0.0008, -0.2702]]), tensor([[ 0.0220,  0.4161, -0.0182, -0.5570]]), tensor([[ 0.0666,  0.5594, -0.1161, -0.9664]]), tensor([[ 0.0072, -0.3637, -0.0030,  0.5993]]), tensor([[ 0.0201, -0.1516,  0.0084,  0.2496]]), tensor([[ 0.0070,  0.0428,  0.0294, -0.0284]]), tensor([[ 0.0872,  0.6251, -0.1373, -1.1648]]), tensor([[ 0.0477, -0.1638, -0.0517,  0.2006]]), None, tensor([[ 0.0170, -0.3469,  0.0134,  0.5450]]), tensor([[ 0.0018,  0.4012,  0.0026, -0.5406]]), tensor([[ 0.0664,  1.1429, -0.0949, -1.7456]]), tensor([[ 0.0324,  0.7514, -0.0437, -1.1269]]), tensor([[ 0.0495,  0.4023, -0.0643, -0.5656]]), None, tensor([[-0.0035, -0.0289, -0.0009, -0.0179]]), tensor([[ 0.0786,  0.4092, -0.1100, -0.7202]]), tensor([[-8.2103e-05, -1.6857e-01,  8.9900e-03,  3.0572e-01]]), tensor([[ 0.0778,  0.7558, -0.1354, -1.2932]]), tensor([[-0.0075,  0.1658,  0.0153, -0.2434]]), None, tensor([[ 0.1161,  1.0184, -0.1905, -1.8354]]), tensor([[ 0.1311,  0.9524, -0.1957, -1.5749]]), tensor([[-0.0142, -0.1678,  0.0171,  0.2878]]), None, tensor([[ 0.0304,  0.2212, -0.0293, -0.2701]]), tensor([[ 0.0320,  0.0275, -0.0286, -0.0086]]), tensor([[-0.0311,  0.2084,  0.0460, -0.2997]]), tensor([[ 0.0326,  0.9475, -0.0615, -1.5019]]), tensor([[ 0.0237,  0.0341, -0.0228, -0.1545]]), tensor([[ 0.0662,  0.4066, -0.0889, -0.6606]]), tensor([[ 0.0217,  0.7916, -0.0249, -1.1278]]), tensor([[ 0.0049, -0.3482,  0.0343,  0.5751]]), tensor([[ 0.0011, -0.1564,  0.0478,  0.3553]]), tensor([[ 0.0106, -0.1687, -0.0092,  0.3096]]), tensor([[ 0.0426,  0.2251, -0.0431, -0.3546]]), tensor([[-0.0381, -0.7464,  0.1220,  1.3433]]), tensor([[ 0.0340, -0.3569, -0.0348,  0.4496]]), tensor([[ 0.0269, -0.1613, -0.0258,  0.1462]]), tensor([[ 0.0410,  0.5918, -0.0906, -0.9288]]), tensor([[-1.7182e-04,  1.6629e-01,  4.5475e-03, -2.5395e-01]]), tensor([[ 0.0471,  0.0306, -0.0502, -0.0758]]), tensor([[ 0.0617,  0.0142, -0.0815, -0.0260]]), tensor([[-0.0070, -0.0291,  0.0144,  0.0447]]), tensor([[ 0.0528,  0.7880, -0.1092, -1.2486]]), tensor([[ 0.1082,  1.1455, -0.1594, -1.8141]]), tensor([[ 0.0192,  0.0436,  0.0093, -0.0460]]), tensor([[-0.0025, -0.2241,  0.0077,  0.3349]]), tensor([[-0.0176,  0.0271,  0.0229,  0.0005]]), tensor([[-0.0314,  0.0140,  0.0464, -0.0220]]), tensor([[-0.0052,  0.0408,  0.0517,  0.0156]]), tensor([[ 0.0051,  0.0261, -0.0046,  0.0227]]), tensor([[ 0.0331,  0.3957, -0.0783, -0.6126]]), tensor([[-0.0189,  0.2072,  0.0285, -0.2725]]), tensor([[ 0.0743,  0.2128, -0.1021, -0.3972]]), tensor([[-0.0102,  0.1981, -0.0154, -0.2638]]), tensor([[-0.0019, -0.0289,  0.0069,  0.0401]]), tensor([[-0.0278, -0.1805,  0.0413,  0.2574]]), tensor([[ 0.0079, -0.1527,  0.0289,  0.2735]]), tensor([[ 0.0929,  0.9524, -0.1613, -1.6250]]), tensor([[-0.0171,  0.2219,  0.0229, -0.2848]]), tensor([[ 0.0056,  0.2213, -0.0042, -0.2714]]), tensor([[ 0.1119,  0.9878, -0.2037, -1.6676]]), tensor([[ 0.0475,  0.9471, -0.0662, -1.4330]]), tensor([[ 0.0064,  0.5566, -0.0195, -0.8990]]), tensor([[0.0001, 0.0261, 0.0003, 0.0223]]), tensor([[ 0.0176,  0.7519, -0.0375, -1.1977]]), tensor([[ 0.0353, -0.1681, -0.0345,  0.2947]]), tensor([[ 0.0134,  0.3942, -0.0491, -0.5779]]), tensor([[-0.0271, -0.5501,  0.1016,  1.0205]]), tensor([[-0.0096,  0.1665,  0.0122, -0.3179]]), tensor([[ 0.0289,  0.4251, -0.0350, -0.7550]])), reward=(tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.]), tensor([1.])))\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[32], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m state \u001b[39m=\u001b[39m next_state\n\u001b[0;32m     27\u001b[0m \u001b[39m# Perform one step of the optimization (on the policy network)\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m optimize_model()\n\u001b[0;32m     30\u001b[0m \u001b[39m# Soft update of the target network's weights\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m#     + (1  )\u001b[39;00m\n\u001b[0;32m     32\u001b[0m target_net_state_dict \u001b[39m=\u001b[39m target_net\u001b[39m.\u001b[39mstate_dict()\n",
            "Cell \u001b[1;32mIn[31], line 12\u001b[0m, in \u001b[0;36moptimize_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m batch \u001b[39m=\u001b[39m Transition(\u001b[39m*\u001b[39m\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mtransitions))\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(batch)\n\u001b[1;32m---> 12\u001b[0m \u001b[39minput\u001b[39;49m()\n\u001b[0;32m     14\u001b[0m \u001b[39m# Compute a mask of non-final states and concatenate the batch elements\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m# (a final state would've been the one after which simulation ended)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m non_final_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mtuple\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m s: s \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     17\u001b[0m                                       batch\u001b[39m.\u001b[39mnext_state)), device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mbool)\n",
            "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py:1187\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1185\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1186\u001b[0m     \u001b[39mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1187\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_request(\n\u001b[0;32m   1188\u001b[0m     \u001b[39mstr\u001b[39;49m(prompt),\n\u001b[0;32m   1189\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parent_ident[\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m   1190\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_parent(\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1191\u001b[0m     password\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1192\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py:1230\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1227\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1228\u001b[0m     \u001b[39m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1229\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInterrupted by user\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1230\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m   1231\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1232\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mInvalid Message:\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    num_episodes = 600\n",
        "else:\n",
        "    num_episodes = 50\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "    # Initialize the environment and get it's state\n",
        "    state, info = env.reset()\n",
        "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    for t in count():\n",
        "        action = select_action(state)\n",
        "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if terminated:\n",
        "            next_state = None\n",
        "        else:\n",
        "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "        # Store the transition in memory\n",
        "        memory.push(state, action, next_state, reward)\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # Perform one step of the optimization (on the policy network)\n",
        "        optimize_model()\n",
        "\n",
        "        # Soft update of the target network's weights\n",
        "        #     + (1  )\n",
        "        target_net_state_dict = target_net.state_dict()\n",
        "        policy_net_state_dict = policy_net.state_dict()\n",
        "        for key in policy_net_state_dict:\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "        target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "        if done:\n",
        "            episode_durations.append(t + 1)\n",
        "            plot_durations()\n",
        "            break\n",
        "\n",
        "print('Complete')\n",
        "plot_durations(show_result=True)\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the diagram that illustrates the overall resulting data flow.\n",
        "\n",
        ".. figure:: /_static/img/reinforcement_learning_diagram.jpg\n",
        "\n",
        "Actions are chosen either randomly or based on a policy, getting the next\n",
        "step sample from the gym environment. We record the results in the\n",
        "replay memory and also run optimization step on every iteration.\n",
        "Optimization picks a random batch from the replay memory to do training of the\n",
        "new policy. The \"older\" target_net is also used in optimization to compute the\n",
        "expected Q values. A soft update of its weights are performed at every step.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
